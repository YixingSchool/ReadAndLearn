1. https://help.aliyun.com/product/27797.html
2. [MaxCompute学习路径](https://help.aliyun.com/learn/learningpath/maxcompute.html)
# 1. 产品简介 
## 1.3 用户必读
https://help.aliyun.com/document_detail/34524.html
> 使用限制汇总
  * SQL开发限制
  * 数据上传下载限制
  * 操作命令限制
  * MapReduce限制
  * 图模型限制
  * 安全配置限制
  * Lightning限制
  * PyODPS限制
# 3. 准备工作 
## 3.5 用户及角色管理
https://help.aliyun.com/document_detail/27805.html
> `项目空间（Project）是MaxCompute实现多租户体系的基础，是您管理数据和计算的基本单位，也是计量和计费的主体`。当用户申请创建项目空间后，该用户即是此空间的所有者（Owner），这个项目空间内的所有对象（表，实例，资源，UDF等）都属于该用户。这就是说，除了Owner之外，任何人都无权访问此项目空间内的对象，除非有Owner的授权许可。
> 对于已在MaxCompute或DateWorks项目中拥有角色的RAM子账号，`请在删除子账号之前解除子账号在项目的角色并在项目空间中删除子账号`。否则子账号会在项目空间中残留，显示为“ p4_xxxxxxxxxxxxxxxxxxxx”且无法在项目空间中移除（不影响项目空间正常功能使用）
# 4. 快速入门 
## 4.3 运行SQL
https://help.aliyun.com/document_detail/27810.html
> 说明
  * `MaxCompute SQL不支持事务、索引及Update/Delete等操作`，同时MaxCompute的SQL语法与Oracle，MySQL有一定差别，您无法将其他数据库中的SQL语句无缝迁移到MaxCompute上来，更多差异请参见与其他SQL语法的差异。
  * 在使用方式上，MaxCompute作业提交后会有几十秒到数分钟不等的排队调度，所以适合处理跑批作业，一次作业批量处理海量数据，不适合直接对接需要每秒处理几千至数万笔事务的前台业务系统。

## 4.5 JAVA UDF开发
https://help.aliyun.com/document_detail/27811.html
1. MaxCompute的UDF包括`UDF、UDAF和UDTF`三种函数。通常情况下，这三种函数被统称为UDF
# 5. 用户指南
## 5.1 基本概念
### 5.1.2 项目空间
https://help.aliyun.com/document_detail/27818.html
> 项目空间（Project）是MaxCompute的基本组织单元，它类似于传统数据库的Database或Schema的概念，是进行多用户隔离和访问控制的主要边界。
> 一个用户可以同时拥有多个项目空间的权限，通过安全授权，`可以在一个项目空间中访问另一个项目空间中的对象`，例如表（Table）、资源（Resource）、函数（Function）和实例（Instance）。
### 5.1.6 生命周期
https://help.aliyun.com/document_detail/55297.html
> MaxCompute表的生命周期（LIFECYCLE），指表（分区）数据从最后一次更新的时间算起，在经过指定的时间后没有变动，则此表（分区）将被MaxCompute自动回收。这个指定的时间就是生命周期。
  * 生命授权单位：days（天），只接受正整数。
  * 非分区表若指定生命周期，自最后一次数据被修改的时间（LastDataModifiedTime）开始计算，经过days天后数据仍未被改动，则此表无需您干预，将会被MaxCompute自动回收（类似drop table操作）。
  * 分区表若指定生命周期，则根据各个分区的LastDataModifiedTime判断该分区是否该被回收。不同于非分区表，分区表的最后一个分区被回收后，该表不会被删除。
  * `生命周期回收都是每天定时启动，扫描全量分区，扫到的时刻，Last modify time超过lifecycle指定的时间才回收`。
  * 假设某个分区表生命周期为1天，该分区数据最后一次被修改的时间是17号15点零分，如果18号的回收扫描在15点前扫到这个表（不到一天），就不会回收上述分区。19号回收扫描时才发现这个表的这个分区Last modify time超过lifecycle指定的时间，这时上述分区会被回收。
  * 生命周期只能设定到表级别，不能再分区级设置生命周期。创建表时即可指定生命周期。
  * `表若不指定生命周期，则表（分区）不会根据生命周期规则被MaxCompute自动回收`。
### 5.1.7 资源
https://help.aliyun.com/document_detail/27822.html
## 5.2 常用命令 
### 5.2.1 常用命令列表
> 使用限制
  * 您在进行资源操作时，请注意`每个资源文件的大小不能超过500M`，`单个SQL、MapReduce任务所引用的资源总大小不能超过2048M`
## 5.3 数据上传下载
### 5.3.1 数据上传/下载概述
https://help.aliyun.com/document_detail/51656.html

> 可以`通过DataHub实时数据通道和Tunnel批量数据通道两种途径进出MaxCompute系统`
> 数据上传/下载的工具主要包括：
* DataHub通道系列
  * OGG插件
  * Flume插件
  * LogStash插件
  * Fluentd插件
* Tunnel通道系列
  * DataWorks
  * DTS
  * Sqoop
  * Kettle插件
  * MaxCompute客户端
> 基于上述丰富的数据上传/下载的工具，可以满足大部分常见的数据上云场景，后续的章节会对工具本身以及`Hadoop数据迁移、数据库数据同步、日志采集等数据上云`的场景进行介绍，为您进行技术方案选型时提供参考。

### 5.3.3 数据上云场景
https://help.aliyun.com/document_detail/51655.html
利用MaxCompute平台的数据上传/下载工具，可以广泛用于各种数据上云的应用场景，本文将介绍几种常见的经典场景。

> Hadoop数据迁移
  * Hadoop数据迁移有两种可选的工具，分别是Sqoop和DataWorks。
    * Sqoop执行时，会在原来的Hadoop集群上执行MR作业，可以分布式地将数据传输到MaxCompute上，效率会比较高
    * 使用DataWorks结合DataX进行Hadoop数据迁移的示例
> 数据库数据同步
  * 数据库数据同步到MaxCompute需要根据数据库的类型和同步策略来选择相应的工具。
    * `离线批量的数据库数据同步：可以选择DataWorks`，支持的数据库种类比较丰富，有MySQL、SQL Server、PostgreSQL等
    * Oracle数据库数据实时同步时，可以选择`OGG`插件工具。
    * RDS数据库数据实时同步时，可以选择`DTS`同步。
> 日志采集
  * 日志采集时，您可以选用Flume、Fluentd、LogStash等工具。

## 5.4 SQL
### 5.4.1 SQL概述
https://help.aliyun.com/document_detail/27860.html

## 5.7 处理非结构化数据
### 5.7.1 前言
https://help.aliyun.com/document_detail/54518.html
> 现阶段 MaxCompute SQL `面对的主要是以 cfile 列格式`，存储在内部 MaxCompute 表格中的结构化数据
> 想要在 MaxCompute 中处理 OSS 上的数据，通常有以下两种做法：
  * 通过 OSS SDK 或者其他工具从 OSS 下载数据，然后再通过 MaxCompute Tunnel 将数据导入表里。
  * 写 UDF，在 UDF 里直接调用 OSS SDK 访问 OSS 数据。
> 本节将介绍一种外部表的功能，支持旨在提供处理除了 MaxCompute 现有表格以外的其他数据的能力。
  * 在这个框架中，通过一条简单的 DDL 语句，即可在 MaxCompute 上创建一张外部表，建立 MaxCompute 表与外部数据源的关联，提供各种数据的接入和输出能力。
  * 创建好的外部表可以像普通的 MaxCompute 表一样使用（大部分场景），充分利用 MaxCompute SQL 的强大计算功能。
# 9. 常见问题
## 9.2 权限问题
1. [项目Owner能否更换为子账号](https://help.aliyun.com/knowledge_detail/40266.html)
> Q：MaxCompute 项目中的 Owner 能否更换为子账号？
  * A：项目的 Owner 不可以更换，谁创建的 Project，谁就是 Owner。您可以将 Admin 的角色赋予子账号。
> Q：与 Owner 相比，Admin 角色有哪些限制？
  * A： 与 Owner 相比，Admin 角色不能进行如下操作：
    * Admin 角色不能将 admin 权限指派给用户。
    * 不能设定项目空间的安全配置。
    * 不能修改项目空间的鉴权模型。
2. [如何删除MaxCompute项目](https://help.aliyun.com/knowledge_detail/94077.html)
> `管理控制台子账号无删除项目权限`，如若操作请联系项目管理员。
Admin 角色所对应的权限不能被修改。
### 9.2.2.功能相关
#### 9.2.2.1 SQL
1. [如何添加/删除列](https://help.aliyun.com/knowledge_detail/40292.html)
> Q：MaxCommpute 中，是否可以添加或删除列？
  * A：`可以添加列，但不可能删除列`。
> Q：如何添加列？
  * A：如果表中已经有了一部分数据，则该新添加列的值是 NULL。
  * 添加列的语法，如下所示：
    * ALTER TABLE table_name ADD COLUMNS (col_name1 type1, col_name2 type2…)
> Q：如何删除列？
  * A：MaxCompute 目前虽不支持删除表的列，但如果您有删除表中的列的需求，可以使用如下方法实现：
    * 通过 SQL 语句创建一张新表。如下所示：
      * `CREATE TABLE new_table_name as SELECT c1,c2,c3 FROM table_name;`
    * 删除原来的表，rename 新表。如下所示：
      * ALTER TABLE new_table_name as RENAME TO table_name;
2. [与标准SQL的主要区别及解决方法](https://help.aliyun.com/knowledge_detail/88101.html)
#### 9.2.2.2 MapReduce
1. [MapReduce常见问题](https://help.aliyun.com/knowledge_detail/40256.html)
> Q：MapReduce 的输入源可以是视图吗？
  * A：`不可以，只能是表`。
> Q：MapReduce 的结果写入到表或分区时，会覆盖还是追加数据？
  * A：`会覆盖掉原有的表数据或者分区数据`。
> Q：不使用 combiner 时，输出正常；使用 combiner 后，reduce 没有输入。
  * A：Reduce 输出单个 record 和 map 输出的 key-value 对不一致，导致上述情况发生。
> Q：MapReduce 执行时，如何在命令行传入多个 resources？
  * A：`用逗号分隔符进行分隔`，输入命令jar -resource resource1,resource2,..即可。
> Q：MapReduce 中是否可以调用 shell 文件？
  * A：`不能，会被沙箱阻挡`。
> Q：reduce.setup 能否读入输入表？
  * A：不能读入输入表，只能读入 cache table。
> Q：Mapper 中输入 table 的每条 Record 数据，是否可以按列名读取？
  * A：Mapper 中输入 table 的每条 Record 数据，不仅可以按序号 record.get(i) 读取，还可以按照列名来获取 record，例如：record.get("size")。
## 9.4 其他问题 
> 1. [Datahub和Tunnel应用场景的区别](https://help.aliyun.com/knowledge_detail/40300.html)
* Datahub 用于实时上传数据的场景，主要用于流式计算的场景。数据上传后会保存到实时表里，后续会在几分钟内通过定时任务的形式同步到离线表里，供离线计算使用。
* Tunnel 用于批量上传数据到离线表里，适用于离线计算的场景。
> 2. [MaxCompute的列数限制](https://help.aliyun.com/knowledge_detail/40299.html)
A：目前 MaxCompute 单表可以存放的最大的列数为 1200 列。如果您的列数超过限制，可以考虑：
* 对数据进行降维，缩减到 1200 以内。
* 修改数据的保存方式，修改成诸如三元组或者稀疏/稠密矩阵。