
# coding: utf-8

# In[ ]:



# 三字经
data1 =  '''人之初，性本善；性相近，习相远。
苟不教，性乃迁；教之道，贵以专。
昔孟母，择邻处；子不学，断机杼。
窦燕山，有义方；教五子，名具扬。
养不教，父之过；教不严，师之惰。
子不学，非所宜；幼不学，老何为？
玉不琢，不成器；人不学，不知义。
为人子，方少时；亲师友，习礼仪。
香九龄，能温席；孝于亲，所当执。
融四岁，能让梨；弟于长，宜先知。
首孝弟，次见闻；知某数，识某文。
一而十，十而百，百而千，千而万。
三才者，天地人。三光者，日月星。
三纲者，君臣义，父子亲，夫妇顺。
曰春夏，曰秋冬；此四时，运不穷。
曰南北，曰西东；此四方，应乎中。
曰水火，木金土；此五行，本乎数。
曰仁义，礼智信；此五常，不容紊。
稻粱菽，麦黍稷；此六谷，人所食。
马牛羊，鸡犬豕；此六畜，人所饲。
曰喜怒，曰哀惧，爱恶欲，七情具。
匏土革，木石金，丝与竹，乃八音。
曰平上，曰去入，此四声，宜调协。
高曾祖，父而身，身而子，子而孙，
自子孙，至玄曾；乃九族，人之伦。
父子恩，夫妇从，兄则友，弟则恭，
长幼序，友与朋，君则敬，臣则忠；
此十义，人所同。

凡训蒙，须讲究；详训诂，明句读。
为学者，必有初；小学终，至四书。
论语者，二十篇；群弟子，记善言。
孟子者，七篇止；讲道德，说仁义。
作中庸，子思笔；中不偏，庸不易。
作大学，乃曾子；自脩齐，至平治。

孝经通，四书熟；如六经，始可读。
诗书易，礼春秋；号六经，当讲求。
有连山，有归藏，有周易，三易详。
有典谟，有训诰，有誓命，书之奥。
我周公，作周礼；着六官，存治体。
大小戴，注礼记；述圣言，礼乐备。
曰国风，曰雅颂；号四诗，当讽咏。
诗既亡，春秋作；寓褒贬，别善恶。
三传者，有公羊，有左氏，有谷梁。
经既明，方读子；撮其要，记其事。
五子者，有荀杨。文中子，及老庄。
经子通，读诸史；考世系，知终始。

自羲农，至黄帝；号三皇，居上世。
唐有虞，号二帝；相揖逊，称盛世。
夏有禹，商有汤，周文武，称三王。
夏传子，家天下；四百载，迁夏社。
汤伐夏，国号商；六百载，至纣亡。
周武王，始诛纣；八百载，最长久。
周辙东，王纲坠；逞干戈，尚游说。
始春秋，终战国；五霸强，七雄出。
嬴秦氏，始兼并；传二世，楚汉争。
高祖兴，汉业建；至孝平，王莽篡。
光武兴，为东汉；四百年，终于献。
魏蜀吴，争汉鼎；号三国，迄两晋。
宋齐继，梁陈承；为南朝，都金陵。
北元魏，分东西；宇文周，与高齐。
迨至隋，一土宇；不再传，失统绪。
唐高祖，起义师；除隋乱，创国基。
二十传，三百载；梁灭之，国乃改。
梁唐晋，及汉周；称五代，皆有由。
炎宋兴，受周禅；十八传，南北混。
十七史，全在兹；载治乱，知兴衰。
读史者，考实录；通古今，若亲目。

口而诵，心而惟；朝于斯，夕于斯。
昔仲尼，师项橐；古圣贤，尚勤学。
赵中令，读鲁论；彼既仕，学且勤。
披蒲编，削竹简；彼无书，且知勉。
头悬梁，锥刺股；彼不教，自勤苦。
如囊萤，如映雪；家虽贫，学不辍。
如负薪，如挂角；身虽劳，犹苦卓。
苏老泉，二十七，始发愤，读书籍。
彼既老，犹悔迟；尔小生，宜早思。
若梁灏，八十二，对大廷，魁多士。
彼既成，众称异；尔小生，宜立志。
莹八岁，能咏诗；泌七岁，能赋碁。
彼颖悟，人称奇；尔幼学，当效之。
蔡文姬，能辨琴；谢道韫，能咏吟。
彼女子，且聪敏；尔男子，当自警。
唐刘晏，方七岁，举神童，作正字。
彼虽幼，身己仕；尔幼学，勉而致；
有为者，亦若是。

犬守夜，鸡司晨；苟不学，曷为人？
蚕吐丝，蜂酿蜜；人不学，不如物。
幼而学，壮而行；上致君，下泽民。
扬名声，显父母；光于前，裕于后。
人遗子，金满籯；我教子，惟一经。
勤有功，戏无益；戒之哉，宜勉力。
'''


# In[ ]:


# 千字文
data2 = '''天地玄黄，宇宙洪荒，日月盈昃，辰宿列张，寒来暑往，秋收冬藏，
 闰余成岁，律吕调阳，云腾致雨，露结为霜，金生丽水，玉出昆冈，
 剑号巨阙，珠称夜光，果珍李柰，菜重芥姜，海咸河淡，鳞潜羽翔，
 龙师火帝，鸟官人皇，始制文字，乃服衣裳，推位让国，有虞陶唐，
 吊民伐罪，周发殷汤，坐朝问道，垂拱平章，爱育黎首，臣伏戎羌，
 遐迩壹体，率宾归王，鸣凤在竹，白驹食场，化被草木，赖及万方，
 盖此身发，四大五常，恭惟鞠养，岂敢毁伤，女慕贞洁，男效才良，
 知过必改，得能莫忘，罔谈彼短，靡恃己长，信使可覆，器欲难量，
 墨悲丝染，诗赞羔羊，景行维贤，克念作圣，德建名立，形端表正，
 空谷传声，虚堂习听，祸因恶积，福缘善庆，尺璧非宝，寸阴是竞，
 资父事君，曰严与敬，孝当竭力，忠则尽命，临深履薄，夙兴温清，
 似兰斯馨，如松之盛，川流不息，渊澄取映，容止若思，言辞安定，
 笃初诚美，慎终宜令，荣业所基，籍甚无竟，学优登仕，摄职从政，
 存以甘棠，去而益咏，乐殊贵贱，礼别尊卑，上和下睦，夫唱妇随，
 外受傅训，入奉母仪，诸姑伯叔，犹子比儿，孔怀兄弟，同气连枝，
 交友投分，切磨箴规，仁慈隐恻，造次弗离，节义廉退，颠沛匪亏，
 性静情逸，心动神疲，守真志满，逐物意移，坚持雅操，好爵自縻，
 都邑华夏，东西二京，背邙面洛，浮渭据泾，宫殿盘郁，楼观飞惊，
 图写禽兽，画彩仙灵，丙舍傍启，甲帐对楹，肆筵设席，鼓瑟吹笙，
 升阶纳陛，弁转疑星，右通广内，左达承明，既集坟典，亦聚群英，
 杜稿钟隶，漆书壁经，府罗将相，路侠槐卿，户封八县，家给千兵，
 高冠陪辇，驱毂振缨，世禄侈富，车驾肥轻，策功茂实，勒碑刻铭，
 磻溪伊尹，佐时阿衡，奄宅曲阜，微旦孰营，桓公匡合，济弱扶倾，
 绮回汉惠，说感武丁，俊乂密勿，多士寔宁，晋楚更霸，赵魏困横，
 假途灭虢，践土会盟，何遵约法，韩弊烦刑，起翦颇牧，用军最精，
 宣威沙漠，驰誉丹青，九州禹迹，百郡秦并，岳宗泰岱，禅主云亭，
 雁门紫塞，鸡田赤城，昆池碣石，巨野洞庭，旷远绵邈，岩岫杳冥，
 治本于农，务资稼穑，俶载南亩，我艺黍稷，税熟贡新，劝赏黜陟，
 孟轲敦素，史鱼秉直，庶几中庸，劳谦谨敕，聆音察理，鉴貌辨色，
 贻厥嘉猷，勉其祗植，省躬讥诫，宠增抗极，殆辱近耻，林皋幸即，
 两疏见机，解组谁逼，索居闲处，沉默寂寥，求古寻论，散虑逍遥，
 欣奏累遣，戚谢欢招，渠荷的历，园莽抽条，枇杷晚翠，梧桐蚤凋，
 陈根委翳，落叶飘摇，游鶤独运，凌摩绛霄，耽读玩市，寓目囊箱，
 易輶攸畏，属耳垣墙，具膳餐饭，适口充肠，饱饫烹宰，饥厌糟糠，
 亲戚故旧，老少异粮，妾御绩纺，侍巾帷房，纨扇圆絜，银烛炜煌，
 昼眠夕寐，蓝笋象床，弦歌酒宴，接杯举觞，矫手顿足，悦豫且康，
 嫡后嗣续，祭祀烝尝，稽颡再拜，悚惧恐惶，笺牒简要，顾答审详，
 骸垢想浴，执热愿凉，驴骡犊特，骇跃超骧，诛斩贼盗，捕获叛亡，
 布射僚丸，嵇琴阮啸，恬笔伦纸，钧巧任钓，释纷利俗，并皆佳妙，
 毛施淑姿，工颦妍笑，年矢每催，曦晖朗曜，璇玑悬斡，晦魄环照，
 指薪修祜，永绥吉劭，矩步引领，俯仰廊庙，束带矜庄，徘徊瞻眺，
 孤陋寡闻，愚蒙等诮，谓语助者，焉哉乎也。'''


# ### Import library

# In[ ]:


import numpy as np
from typing import Optional, Tuple
import time
import tensorflow as tf
from tensorflow.contrib import rnn
import matplotlib.pyplot as plt


# ### Preprocessing data

# In[ ]:


#preprocessing data
import re
data_org = '。'.join([d.strip() for d in re.split(r'，|；|。|？', data1)])
#data_org = '。'.join([d.strip() for d in re.split(r'，|；|。|？', data2)])
#data_org = data_org[0:256]
# add traiing data 
data=data_org+data_org[0:256]


# ### Analysis the data sets

# In[7]:



print(data)

chars = list(set(data_org))
data_size, vocab_size = len(data_org), len(chars)
print ('data has %d characters, %d unique.' % (data_size, vocab_size))
print ("data has %d secentance" % (len(re.split(r'。',data_org))))
char_to_ix = { ch:i for i,ch in enumerate(chars) }
ix_to_char = { i:ch for i,ch in enumerate(chars) }

print(chars[:24])
print(ix_to_char)
first_4=[char_to_ix[x] for x in data[0:4]]
print("First sentence", data[0:4],first_4)


# ### Hyperparameters

# In[ ]:


# Parameters
learning_rate = 0.005
training_iters = 5000
batch_size = 1
display_step = 100
target_loss=0.01


# In[ ]:


## Network Parameters
# 
n_input = vocab_size # 
n_steps=4
n_hidden = 250  # hidden layer num of features
n_outputs = 1


# ### Build RNN model

# In[ ]:




def RNN(x, weights, biases, n_input, n_steps, n_hidden, n_output):

    # Prepare data shape to match `rnn` function requirements
    # Current data input shape: (batch_size, n_steps, n_input)
    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)

    # Permuting batch_size and n_steps
    x = tf.transpose(x, [1, 0, 2])
    # Reshaping to (n_steps*batch_size, n_input)
    x = tf.reshape(x, [-1, n_input])
    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)
    # x: [n_steps, batch_size, n_input]
    x = tf.split(x, n_steps, axis=0)

    # Define a lstm cell with tensorflow
    lstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, forget_bias=1.0)
    #lstm_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)

    # Get lstm cell output
   
    reset_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)
    
    # [n_steps, hidden]
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)

    
    """
    static_rnn(cell, inputs):
        state = cell.zero_state(...)
        outputs = []
        for input_ in inputs:
            output, state = cell(input_, state)
            outputs.append(output)
        return (outputs, state)
    """

    # Linear activation, using rnn inner loop last output
    # Note: use the last step in outputs
    pred=[]
    for s in range(n_output):
        output=outputs[-1]
        pred1 = tf.nn.bias_add(tf.matmul(output, weights['out'][:,:,s]), biases['out'][:,s])
        pred.append(pred1)
    return pred, reset_state


# ### Generate sample data

# In[ ]:



def one_hot(x,size):
   y=np.zeros(size)
   y[x]=1.
   return y

def generate_sample(data, p, batch_size, n_steps, n_input, n_outputs):
   x_=np.zeros((batch_size, n_steps, vocab_size))
   y_=np.zeros((batch_size, n_outputs))
   for b in range(batch_size):
       inputs = np.array([one_hot(char_to_ix[ch],vocab_size) for ch in data[p+b:p+b+n_steps]]).reshape(-1)
       x_[b,:,:]=inputs.reshape(n_steps,-1)
       y_[b,:] = np.array([char_to_ix[ch] for ch in data[p+b+n_steps:p+b+n_outputs+n_steps]]).reshape(-1)    
   return x_, y_
       


# ### convert a one-hot vector to character string

# In[ ]:


def print_char(x, size):
    x=x.reshape(size, -1)

    t=''.join([ix_to_char[np.where(s==1)[0][0]] for s in x])
    
    return t


# ### Plot the curve

# In[ ]:



def plot_curve(x):
    plt.figure(figsize=(8,6))
    plt.title("Loss per interation")
    plt.plot(x,linestyle="-", label='')
    plt.legend(loc='upper left',title="Loss")
    plt.show()    


# ### Training model

# In[14]:


tf.reset_default_graph()

# tf Graph input
x = tf.placeholder("float", [None, n_steps, n_input], name='x')
y = tf.placeholder("int32", [None, n_outputs], name='y')

# Define weights
weights = {
    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size, n_outputs]))
}
biases = {
    'out': tf.Variable(tf.random_normal([vocab_size,n_outputs]))
}

loss=0.0
#(batch_size, n_steps, n_input)
pred, reset_state = RNN(x, weights, biases, n_input, n_steps, n_hidden, n_outputs)

for s in range(n_outputs):
    one_hot_y = tf.one_hot(y[:,s], vocab_size)
    pred1=pred[s]
    loss+= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred1, labels=one_hot_y), name='cost')


optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
    
# Initializing the variables
init = tf.global_variables_initializer()
ixes = []
loss_log=[]
# Launch the graph
saver = tf.train.Saver()
with tf.Session() as sess:
    sess.run(init)
    step = 1
    p=0
    seed=0
    total_loss=0.0
    # Keep training until reach max iterations
    while step < training_iters:
        if step == 1: 
            p = seed #go from start of data
            sess.run(reset_state)
        if p >= data_size: 
            print("restart")
            print("iter loss=%.5f"%(total_loss/(data_size)))
            if (total_loss/data_size<target_loss):
                print("early terminate because of meeting the target, quit training process")
                break
            total_loss=0.0
            p = seed #go from start of data
 
            #state_value=sess.run(states, feed_dict={x: batch_x, y: batch_y})
            #seed+=1
        batch_x, batch_y = generate_sample(data, p, batch_size, n_steps, n_input, n_outputs)
        #print(print_char(batch_x, n_steps), ix_to_char[batch_y[0][0]])
        # Run optimization op (backprop)
        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
        p+=1
        loss_value = sess.run(loss, feed_dict={x: batch_x, y: batch_y})
        total_loss+=loss_value
        loss_log.append(loss_value)

        if step % display_step == 1:
            # Calculate batch loss
            print("Iter " + str(step) + ", Minibatch Loss= " +
                  "{:.6f}".format(loss_value))
        step += 1
        
    print("Optimization Finished! %d Steps"%step)
    print("final loss=%.5f"%(total_loss/(data_size)))
    
    save_path = saver.save(sess, "/tmp/model.ckpt")
    print("Model saved")


# In[15]:


plot_curve(loss_log)


# In[16]:


plot_curve(loss_log)


# In[17]:


#tf.reset_default_graph()
with tf.Session() as sess:    
    saver.restore(sess, "/tmp/model.ckpt")
    seed_ix=20
    x_ = np.zeros(vocab_size*n_steps)
    x_=x_.reshape(1,n_steps,-1)
    x_[0,:,seed_ix] = 1
    
    x_[0,:,:]=[one_hot(i, vocab_size) for i in first_4]
    y_=np.zeros(n_outputs)
    print(y_.shape, n_outputs)
    
    y_=y_.reshape((-1,n_outputs))
    
    print(y_.shape, n_outputs)
    
    print(first_4)
    for t in range(4*100):
        ps =sess.run(pred,feed_dict={x: x_, y: y_})
        p = ps[0]
        p = np.exp(p) / np.sum(np.exp(p), axis=1, keepdims=True)
        
        #print(p.shape)
        # generate a sequence which has the probability of p, p shape (vocab_size)
        ix = np.random.choice(range(vocab_size),p=p.ravel())
        #ix = np.argmax(p)
        #print(print_char(x_),  ix_to_char[ix])
        if 1:
            for s in range(1,n_steps,1):
                x_[:,s-1,:]=x_[:,s,:]

            x_[0,-1,:] = 0
            x_[0,-1,ix] = 1
        else:
            #debug purpose
            x_[0,:,:]=[one_hot(char_to_ix[x], vocab_size) for x in data[4*t:4*t+4]]
        #print(x_)
        ixes.append(ix)
        if(ix_to_char[ix]==','):
            break;

    #print(ixes)
    
    generated_sentence=''.join([ix_to_char[x] for x in ixes])
    print("Generated sentence:")
    print(generated_sentence)


# In[18]:


#tf.reset_default_graph()
import re
with tf.Session() as sess:    
    saver.restore(sess, "/tmp/model.ckpt")
    seed_ix=20
    ix = np.zeros(vocab_size*n_outputs)
    ix=ix.reshape(1,n_outputs,-1)
    x_ = np.zeros(vocab_size*n_steps)
    x_=x_.reshape(1,n_steps,-1)
    
    y_=np.zeros(n_outputs)    
    y_=y_.reshape((-1,n_outputs))
    
    sen=re.split(r'，|；|。',data)
    np.random.seed(0)
    for t in range(16):
        r=np.random.randint(int(data_size/4)-n_steps)
        sen1=sen[r]+'。'
        #print (sen[r])
        x_[0,:,:] = [one_hot(char_to_ix[ch],vocab_size) for ch in sen1]
        ps =sess.run(pred,feed_dict={x: x_, y: y_})
        #print(x_)
        prob = [np.exp(p) / np.sum(np.exp(p), axis=1, keepdims=True) for p in ps]
        ix[0,:,:] = [one_hot(np.argmax(p), vocab_size) for p in prob]
        #print(ix)
            
        hint=print_char(x_,n_steps)
        answer=print_char(ix, n_outputs)
        print("Hint:", hint)
        print("Answer:", answer)

