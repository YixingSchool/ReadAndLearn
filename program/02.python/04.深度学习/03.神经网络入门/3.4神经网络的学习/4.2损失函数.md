
1. 损失函数
    1. 神经网络的学习通过某个指标表示现在的状态，然后，以这个指标为基准，寻找最优权重参数
    2. 神经网络的学习中所用的指标被称为`损失函数(Loss Function)`
    3. 损失函数可以使用任意函数，但一般使用`均方误差和交叉熵误差`
    4. 损失函数是表示神经网络性能的`恶劣程度`，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致

# 1. 均方误差mean squared error

$$ E = \frac{1}{2}\sum_k{(y_k - t_k)}^2 $$
$ y_k表示神经网络的输出, t_k表示监督数据，k表示数据的维度 $

```py
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

1. one-hot表示
    1. 将正确解标签表示为1，其他标签表示为0的表示方法称为`one-hot表示`

# 2. 交叉熵误差 cross entropy error

$$ E = -\sum_k{t_{k}logy_{k}} $$

$ t_k $中只有正确解标签的索引为1，其他均为0。因此，只计算对应正确解标签的输出的自然对数

```py
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
# 0.51082545709933802

y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
cross_entropy_error(np.array(y), np.array(t))
# 2.3025840929945458
```

添加一个微小值delta可以防止负无限大( np.log(0) = -inf )的发生

# 3. mini-batch学习

$$ E = -\frac{1}{N}\sum_n\sum_kt_{nk}logy_{nk} $$

1. 求所有训练数据的损失函数的总和
    1. 最后除以N进行正规化
2. mini-batch学习
    1. 从训练数据中选取一批数据，然后对每个mini-batch进行学习

```py
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize=True, one_hot_label=True)
print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000, 10)
```

# 4. mini-batch版交叉熵误差的实现

```py
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

# 参考

1. 深度学习入门.基于Python的理论与实现.斋藤康毅.2018
    1. 4.2损失函数
2. https://github.com/oreilly-japan/deep-learning-from-scratch