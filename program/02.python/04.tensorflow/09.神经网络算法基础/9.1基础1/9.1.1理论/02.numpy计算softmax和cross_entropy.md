

## 1. numpy 


```py
import numpy as np
def softmax(logits):
    return np.exp(logits)/np.sum(np.exp(logits))

def cross_entropy(label, prediction_softmax):
    result = np.sum(-1*np.log(prediction_softmax)*label)
    return result

label = np.array([[0, 1]],dtype=np.float32)

prediction1 = np.array([[0.9, 0.1]], dtype=np.float32)
c1 = cross_entropy(label, softmax(prediction1))
print(c1) # 1.1711006
prediction2 = np.array([[0.3, 0.7]], dtype=np.float32)
c2 = cross_entropy(label, softmax(prediction2))
print(c2) # 0.5130153
```


## 2. tensorflow softmax

```py
import tensorflow as tf

s=tf.constant([1,0,2],dtype=tf.float32)
sm=tf.nn.softmax(s)

with tf.Session()as sess:
   print(sess.run(sm))
   print(sess.run(tf.argmax(sm)))
```

## 参考

1. [tensorflow学习之softmax使用详解](https://blog.csdn.net/u013230189/article/details/82835717)