
# 1. 恒等函数和softmax函数

1. 神经网络需要根据情况改变输出层的激活函数
    1. 回归问题用恒等函数
        1. 回归问题是根据某个输入预测一个（连续的）数值的问题
    2. 分类问题用softmax函数
        1. 分类问题是数据属于哪一个类别的问题

2. ![图3.22softmax函数.png](图3.22softmax函数.png)
    1. softmax函数公式
        1. exp(x)是表示 $e^{x}$ 的指数函数（e是纳皮尔常数2.7182 . . .）。
        2. 假设输出层共有n个神经元，计算第k个神经元的输出 $y_{k}$。
        3. softmax函数的分子是输入信号$a_k$的指数函数，分母是所有输入信号的指数函数的和
    2. softmax函数图
        1. softmax函数的输出通过箭头与所有的输入信号相连
        2. 输出层的各个神经元都受到所有输入信号的影响

3. softmax函数python实现

```py
a = np.array([0.3, 2.9, 4.0])
exp_a = np.exp(a) # 指数函数
print(exp_a) # [ 1.34985881 18.17414537 54.59815003]
sum_exp_a = np.sum(exp_a) # 指数函数的和
print(sum_exp_a) # 74.1221542102

y = exp_a / sum_exp_a
print(y) # [ 0.01821127 0.24519181 0.73659691]
```

4. 定义softmax通用函数

```py
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    
    return y
```

# 2. softmax函数的溢出问题

1. 溢出问题
    1. 指数函数的值很容易变得非常大。对这些超大值之间进行除法运算，结果会出现`不确定`的情况
    2. 在进行softmax指数函数运算时，加上（或者减去）某个常数并不会改变运算的结果

```py
a = np.array([1010, 1000, 990])
np.exp(a) / np.sum(np.exp(a)) # softmax函数的运算 array([ nan, nan, nan]) # 没有被正确计算
c = np.max(a) # 1010
a - c # array([ 0, -10, -20])
np.exp(a - c) / np.sum(np.exp(a - c)) # array([ 9.99954600e-01, 4.53978686e-05, 2.06106005e-09])

def softmax(a):
    c = np.max(a) 
    exp_a = np.exp(a - c) # 溢出对策，减去输入信号中的最大值
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

# 3. softmax函数的特征

```py
a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y) # [ 0.01821127 0.24519181 0.73659691]
np.sum(y) # 1.0
```

1. softmax
    1. softmax函数的输出是0.0到1.0之间的实数
    2. softmax函数的输出值的总和是1
    3. softmax函数的输出可以解释为`概率`

# 4. 输出层的神经元数量

# 参考

1. [Markdown语法和LaTeX数学记号速查手册 ](http://www.domuse.com/markdown-and-latex-equation-handbook.html)
2. 深度学习入门.基于Python的理论与实现.斋藤康毅.2018 -> 3.5.1恒等函数和softmax函数