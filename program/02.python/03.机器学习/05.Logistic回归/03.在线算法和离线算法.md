# 1. 在线算法和离线算法 - Mercury_Lc的博客 - CSDN博客 https://blog.csdn.net/Mercury_Lc/article/details/82117998

离线算法
算法设计策略都是基于在执行算法前输入数据已知的基本假设，也就是说，对于一个离线算法，在开始时就需要知道问题的所有输入数据，而且在解决一个问题后就要立即输出结果，通常将这类具有问题完全信息前提下设计出的算法成为离线算法( off line algorithms)

 
例题可供参考

http://blog.csdn.net/xj2419174554/article/details/10238557

http://blog.csdn.net/xj2419174554/article/details/9567989

在线算法
在计算机科学中，一个在线算法是指它可以以序列化的方式一个个的处理输入，也就是说在开始时并不需要已经知道所有的输入。相对的，对于一个离线算法，在开始时就需要知道问题的所有输入数据，而且在解决一个问题后就要立即输出结果。例如，选择排序在排序前就需要知道所有待排序元素，然而插入排序就不必。

因为在线算法并不知道整个的输入，所以它被迫做出的选择最后可能会被证明不是最优的，对在线算法的研究主要集中在当前环境下怎么做出选择。对相同问题的在线算法和离线算法的对比分析形成了以上观点。如果想从其他角度了解在线算法可以看一下 流算法（关注精确呈现过去的输入所使用的内存的量），动态算法（关注维护一个在线输入的结果所需要的时间复杂度）和在线机器学习。

 

转自：http://blog.csdn.net/xiaofengcanyuexj


# 2. 机器学习中的在线学习与离线学习 - 膝盖走路JYM的博客 - CSDN博客 https://blog.csdn.net/a133521741/article/details/79221015

一直以来对这个有所疑惑，所里师姐和师兄的解释好像和论文中的在线离线有所不同。现在国内外有这么几种理解方式。

我就在这边给自己做个小笔记吧。有不对的地方望予以指正，本人必虚心改正。

在线学习和离线学习(online learning and offline learning)

理解方式一：

在这一次训练中：

 在线学习：一个数据点训练完了直接更新权重（而不是一个batch），看到了没？直接更新权重，这里有危害处！（我们无法得知这一次的更新权重是正确的还是错误的，如果恰恰是错误的一次更新，那么我们的模型会有可能渐渐地走向错误方向，残差出现）

 离线学习：一个batch训练完才更新权重，这样的话要求所有的数据必须在每一个训练操作中（batch中）都是可用的，个人理解，这样不会因为偶然的错误把网络带向极端。

 这种理解方式在国外论文中出现比较多，国外称为online and batch learning.离线就是对应batch learning.这两种方式各有优点，在线学习比较快，但是有比较高的残差，离线（batch）学习能降低残差。

理解方式二：

 在离线学习中，所有的训练数据在模型训练期间必须是可用的。只有训练完成了之后，模型才能被拿来用。简而言之，先训练，再用模型，不训练完就不用模型。

 在在线学习中，恰恰相反，在线算法按照顺序处理数据。它们产生一个模型，并在把这个模型放入实际操作中，而不需要在一开始就提供完整的的训练数据集。随着更多的实时数据到达，模型会在操作中不断地更新。

理解方式三：

 这一种是知乎一位兄弟的解释。我在实际代码中基本没见过。可能还是学习太少了。暂时收录下吧。

作者：IvonChen
 链接：https://www.zhihu.com/question/35607456/answer/150511176
 来源：知乎
 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。


 试答

 online training：

 你有一个样本 你把第一条带入训练 调整权重 再把这一条带进去一次 重复多次 直至误差率很小

 然后再带入下一条 直至跑完整个样本

 offline training：

 你有一个样本 你把第一条带入训练 调整权重 然后带入下一条 直至跑完整个样本 这个时候的误差率可能不让你满意 于是你 把整个样本又做了上述操作 直到误差很小

 offline其实和batch更相似


 假定这个样本有m条记录

 offline会训练m的整数倍次数

 online不知道会训练多少次 可能以一条记录训练了10次 第二条8次 第三条1次……


 分享一下伪代码：

 online：

 initialize all weights to random value

 for t in training_set:

 repeat:

 compute train_error for t

 adjust weights base on train_error

 until error rate is very small or error rate variation stops


 offline:

 initialize all weights to random value

 repeat:

 for t in training_set:

 compute train_error for t

 adjust weights base on train_error

 until error rate is very small or error rate variation stops


————————————————
版权声明：本文为CSDN博主「膝盖走路JYM」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/a133521741/article/details/79221015