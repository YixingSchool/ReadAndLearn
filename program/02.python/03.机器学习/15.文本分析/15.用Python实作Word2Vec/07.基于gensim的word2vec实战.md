基于gensim的word2vec实战 - 简书 https://www.jianshu.com/p/5f04e97d1b27

word2vec 介绍：将词汇转化为机器能够读懂的向量～向量每个维度的含义，无意义.......

训练数据：wiki 开放的中文数据：zhwiki_2017_03.clean，1G大小

首先，观测当前训练数据样式：



可以看出，数据样本为，关键字－详细解释，两个维度。共有92W条数据。

这里采用jieba工具进行分词。
步骤１：数据预处理
先用正则去除文本中的标点符号，在结合结巴分词工具进行分词。
这里默认采用结巴的精准分词。也可以根据不同情况，采用全分词策略。

r = re.compile("[\s+\.\!\/_,$%^*(+\"\']+|[+——！；「」》:：“”·‘’《，。？、~@#￥%……&*（）()]+")
 #以下两行过滤出中文及字符串以外的其他符号
sentence = r.sub('',str(content[i][0]))
seg_list = jieba.cut(sentence)
停用词过滤

import pandas as pd
stopwords = pd.read_table('stopwords.txt',header = None).iloc[:,:].values

def stopword_filter(stopwords,seq_words):
    filter_words_list = []
    #停用词过滤
    for word in seq_words:
        if word not in stopwords:
            filter_words_list.append(word)
    
    return filter_words_list
r = re.compile("[\s+\.\!\/_,$%^*(+\"\']+|[+——！；「」》:：“”·‘’《，。？、~@#￥%……&*（）()]+")

#for i in range(content.shape[0]):
head = ["分词结果"]
to_csv_content = []

for i in range(content.shape[0]):
    #以下两行过滤出中文及字符串以外的其他符号
    sentence = r.sub('',str(content[i][0]))
    seg_list = jieba.cut(sentence)
#     to_csv_content.append(" ".join(seg_list))
    seq_words = " ".join(seg_list).split(' ');
    to_csv_content.append(stopword_filter(stopwords,seg_list))
    
    if i % 100000 == 0 and i != 0:
        df = pd.DataFrame (to_csv_content , columns = head)
        df.to_csv("bf_stopword_filter/seq2_stopword_filter_"+ str(i) + ".csv" , encoding = "utf-8")
        to_csv_content.clear()
        print("finish_"+str(i))
    
df = pd.DataFrame(to_csv_content , columns = head)
df.to_csv ("bf_stopword_filter/seq2_stopword_filter__929050.csv" , encoding = "utf-8")
to_csv_content.clear()
print("finish_929050")
这里我将原始数据拆分为１0份，这里随意～
步骤２
word2vec 模型训练
根据 文档 提示

Produce word vectors with deep learning via word2vec’s “skip-gram and CBOW models”, using either hierarchical softmax or negative sampling

word2vec总共有两种训练方法，cbow 和 skip-gram,以及两种方式的损失函数构造，具体原理这里不再赘述。
有个值得注意的地方就是：

Make sure you have a C compiler before installing gensim, to use optimized (compiled) word2vec training

否则采用gensim框架训练模型的时候会事件会很长。 = =！

训练模型的方法：

model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)
模型保存以及模型加载

model.save(model_name)
model = Word2Vec.load(model_name)
继续之前的代码，读取之前分好词的文本,仅为了实验，这里只获取10000条数据,设置训练２００维，其余参数默认：

import pandas as pd 
data_100000 = pd.read_csv('seq2_100000.csv', header = 0) 
content = data_100000.iloc[:10000,1:2].values
train_content = []
for item in content: 
    train_content.append(item[0]).split(' '))

from gensim.models import word2vec 
import time
start = time.clock()
model=word2vec.Word2Vec(train_content, size=200)
end = time.clock()
print('Running time: %s Seconds'%(end-start))
测试

y2 = model.wv.similarity(u"时期", u"种类")
print(y2)

for i in model.wv.most_similar(u"戏剧"):
    print (i[0],i[1])
0.028777361728733544
话剧 0.9370647668838501
舞蹈 0.8956470489501953
音乐家 0.8940117359161377
喜剧 0.8896620273590088
艺术家 0.8895379304885864
艺术作品 0.883725106716156
歌唱 0.8832190036773682
儿童文学 0.8812234997749329
流行音乐 0.8799906969070435
音乐 0.877548336982727
选取其中一些词汇，采用TSNE 进行降维可视化，不难发现，意思相近的词汇在空间中的位置也就相对贴近：

X_tsne = TSNE(n_components=2,learning_rate=100).fit_transform(model.wv[random_word])

from matplotlib.font_manager import *  
import matplotlib.pyplot as plt 
#解决负号'-'显示为方块的问题  
plt.figure(figsize=(14, 8)) 
myfont = FontProperties(fname='/usr/share/fonts/wqy-zenhei/wqy-zenhei.ttc')

plt.scatter(X_tsne[:,0],X_tsne[:,1])
for i in range(len(X_tsne)):
    x=X_tsne[i][0]
    y=X_tsne[i][1]
    plt.text(x , y ,random_word[i], fontproperties= myfont,size = 16)
 
plt.show()

到此模型训练就算完成了，这里对gensim中word2vec中的参数做一下我认为比较重要的参数，即可能会用到的参数。
sg (int {1, 0}) : 　表示训练的方法　如果是１则采用skip-gram,否则采用cbow,默认为0
size : 词向量的维度。
min_count：低于设置词频的词会被忽略。
workers: 设置并发
hs (int {1,0}) : 如果是１则采用hierarchical softmax 方式估计. 如果是 0采用.“负采样”（negative sampling）方式训练。

简要总结：如果训练数据量较大的情况下，采用skip-gram 的效果会更加。

model.save('/tmp/mymodel')  
model.save_word2vec_format('/tmp/mymodel.txt',binary=False)   
前一组方法保存可以在读取后追加训练
后一组方法保存不能追加训练

参考资料：
gensim word2vec模块：
https://radimrehurek.com/gensim/models/word2vec.html
https://rare-technologies.com/word2vec-tutorial/
jieba使用说明：
http://www.oss.io/p/fxsjy/jieba
warning解决方案：
https://blog.csdn.net/bychahaha/article/details/47908295
gensim word2vec模块增量训练：
https://blog.csdn.net/qq_19707521/article/details/79169826

作者：一心一意弄算法
链接：https://www.jianshu.com/p/5f04e97d1b27
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。