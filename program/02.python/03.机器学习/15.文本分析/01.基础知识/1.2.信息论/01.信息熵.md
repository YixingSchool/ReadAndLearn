

# 1. 直观解释

1. 信息熵用来衡量信息量的大小
    1. 若不确定性越大，则信息量越大，熵越大
    2. 若不确定性越小，则信息量越小，熵越小
2. 举例：比如A班对B班，胜率一个为x，另一个为1-x
    1. 信息熵为 -(xlogx + (1-x)log(1-x))
    2. 求导后容易证明x=1/2时取得最大，最大值为2
    3. 也就是说两者势均力敌时，不确定性最大，熵最大。

# 参考

1. [每日一个机器学习算法——信息熵](https://www.cnblogs.com/ShaneZhang/p/3970176.html)
2. [信息熵到底是什么](https://blog.csdn.net/saltriver/article/details/53056816)