
# 参考

1. (入门-已读) 基于 Gensim 的 Word2Vec 实践 - 某熊的全栈之路 - SegmentFault 思否
    1. 人工智能与深度学习实战 https://github.com/wx-chevalier/AIDL-Series
    2. https://segmentfault.com/a/1190000008173404
    3. https://zhuanlan.zhihu.com/p/24961011
2. (重点-需复习) 机器学习：gensim之Word2Vec 详解 - 千千寰宇 - 博客园 https://www.cnblogs.com/johnnyzen/p/10900040.html
3. Getting Started with Word2Vec and GloVe in Python http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python
4. 基于gensim的word2vec实战 - 简书 https://www.jianshu.com/p/5f04e97d1b27
6. 官方文档 gensim word2vec模块：
https://radimrehurek.com/gensim/models/word2vec.html
https://rare-technologies.com/word2vec-tutorial/
7. jieba使用说明：http://www.oss.io/p/fxsjy/jieba
8. warning解决方案：https://blog.csdn.net/bychahaha/article/details/47908295
9. gensim word2vec模块增量训练：https://blog.csdn.net/qq_19707521/article/details/79169826
10. word2vec词向量中文语料处理(python gensim word2vec总结） - shuihupo的博客 - CSDN博客 https://blog.csdn.net/shuihupo/article/details/85162237
11. Gensim之Word2Vec使用手册 - 空字符 - CSDN博客 https://blog.csdn.net/The_lastest/article/details/81734980
12. https://github.com/RaRe-Technologies/gensim
    1. https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/word2vec.py


# Word2Vec
基于 Gensim 的 Word2Vec 实践，从属于笔者的程序猿的数据科学与机器学习实战手册，代码参考gensim.ipynb。推荐前置阅读Python语法速览与机器学习开发环境搭建，Scikit-Learn 备忘录。

Word2Vec Tutorial

Getting Started with Word2Vec and GloVe in Python

# 1. 模型创建
Gensim中 Word2Vec 模型的期望输入是进过分词的句子列表，即是某个二维数组。这里我们暂时使用 Python 内置的数组，不过其在输入数据集较大的情况下会占用大量的 RAM。Gensim 本身只是要求能够迭代的有序句子列表，因此在工程实践中我们可以使用自定义的生成器，只在内存中保存单条语句。

```py
# 例子1
# 引入 word2vec
from gensim.models import word2vec
# 引入日志配置
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
# 引入数据集
raw_sentences = ["the quick brown fox jumps over the lazy dogs","yoyoyo you go home now to sleep"]
# 切分词汇
# sentences= [s.encode('utf-8').split() for s in sentences]
sentences= [s.split() for s in raw_sentences]
# 构建模型
model = word2vec.Word2Vec(sentences, min_count=1)
# 进行相关性比较
model.similarity('dogs','you')
# -0.14555506

# 例子2
model = gensim.models.Word2Vec.load('fzhxx_model') #加载模型
a = model.most_similar(positive=['积极词1','积极词2'],negative=['消极词1','消极词2'],topn=40)  #取最符合的40个词
b = model.similarity('词1', '词2') #两个词的相似度
c = model.doesnt_match(['词1','词2','词3']) #最不一样的词
d = model['词汇名'] #词向量
```
1. 这里我们调用Word2Vec创建模型实际上会对数据执行两次迭代操作
    1. 第一轮操作会统计词频来构建内部的词典数结构
    2. 第二轮操作会进行神经网络训练
2. 而这两个步骤是可以分步进行的，这样对于某些不可重复的流（譬如 Kafka 等流式数据中）可以手动控制：

```py
# 在线训练，分步训练
# model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet
model = word2vec.Word2Vec(iter=1)
model.build_vocab(some_sentences)   # can be a non-repeatable, 1-pass generator
model.train(other_sentences)   # can be a non-repeatable, 1-pass generator

from gensim.models import word2vec
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
raw_sentences = ["the quick brown fox jumps over the lazy dogs","yoyoyo you go home now to sleep"]
sentences= [s.split() for s in raw_sentences]
model = word2vec.Word2Vec(iter=1,min_count=1)
model.build_vocab(sentences)
model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)
model.similarity('dogs','you')
```

# 1.1 报错：python 在训练word2vec模型时，弹出RuntimeError:you must first build vocabulary before training the model
解决方法：
训练语料不满足要求（词太少了， 没有达到默认的最少计数 mincount），换一个合适的语料或者改一下 min_count
voc_vec = word2vec.Word2Vec(vocab, min_count=1)


# 2. Word2Vec 参数
1. min_count
model = Word2Vec(sentences, min_count=10) # default value is 5
在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置min_count参数进行控制。一般而言，合理的参数值会设置在0~100之间。
2. size
size参数主要是用来设置神经网络的层数，Word2Vec 中的默认值是设置为100层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。
model = Word2Vec(sentences, size=200) # default value is 100
3. workers
workers参数用于设置并发训练时候的线程数，不过仅当Cython安装的情况下才会起作用：
model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization

详情
sentences (iterable of iterables, optional) – 供训练的句子，可以使用简单的列表，但是对于大语料库，建议直接从磁盘/网络流迭代传输句子。参阅word2vec模块中的BrownCorpus，Text8Corpus或LineSentence。
corpus_file (str, optional) – LineSentence格式的语料库文件路径。
size (int, optional) – word向量的维度。
window (int, optional) – 一个句子中当前单词和被预测单词的最大距离。
min_count (int, optional) – 忽略词频小于此值的单词。
workers (int, optional) – 训练模型时使用的线程数。
sg ({0, 1}, optional) – 模型的训练算法: 1: skip-gram; 0: CBOW.
hs ({0, 1}, optional) – 1: 采用hierarchical softmax训练模型; 0: 使用负采样。
negative (int, optional) – > 0: 使用负采样，设置多个负采样(通常在5-20之间)。
ns_exponent (float, optional) – 负采样分布指数。1.0样本值与频率成正比，0.0样本所有单词均等，负值更多地采样低频词。
cbow_mean ({0, 1}, optional) – 0: 使用上下文单词向量的总和; 1: 使用均值，适用于使用CBOW。
alpha (float, optional) – 初始学习率。
min_alpha (float, optional) – 随着训练的进行，学习率线性下降到min_alpha。
seed (int, optional) – 随机数发生器种子。
max_vocab_size (int, optional) – 词汇构建期间RAM的限制; 如果有更多的独特单词，则修剪不常见的单词。 每1000万个类型的字需要大约1GB的RAM。
max_final_vocab (int, optional) – 自动选择匹配的min_count将词汇限制为目标词汇大小。
sample (float, optional) – 高频词随机下采样的配置阈值，范围是(0,1e-5)。
hashfxn (function, optional) – 哈希函数用于随机初始化权重，以提高训练的可重复性。
iter (int, optional) – 迭代次数。
trim_rule (function, optional) – 词汇修剪规则，指定某些词语是否应保留在词汇表中，修剪掉或使用默认值处理。
sorted_vocab ({0, 1}, optional) – 如果为1，则在分配单词索引前按降序对词汇表进行排序。
batch_words (int, optional) – 每一个batch传递给线程单词的数量。
compute_loss (bool, optional) – 如果为True，则计算并存储可使用get_latest_training_loss()检索的损失值。
callbacks (iterable of CallbackAny2Vec, optional) – 在训练中特定阶段执行回调序列。


# 3. 外部语料集
在真实的训练场景中我们往往会使用较大的语料集进行训练，譬如这里以 Word2Vec 官方的text8为例，只要改变模型中的语料集开源即可：
```py
sentences = word2vec.Text8Corpus('text8')
model = word2vec.Word2Vec(sentences, size=200)
```
这里语料集中的语句是经过分词的，因此可以直接使用。笔者在第一次使用该类时报错了，因此把 Gensim 中的源代码贴一下，也方便以后自定义处理其他语料集：

```py
class Text8Corpus(object):
    """Iterate over sentences from the "text8" corpus, unzipped from http://mattmahoney.net/dc/text8.zip ."""
    def __init__(self, fname, max_sentence_length=MAX_WORDS_IN_BATCH):
        self.fname = fname
        self.max_sentence_length = max_sentence_length

    def __iter__(self):
        # the entire corpus is one gigantic line -- there are no sentence marks at all
        # so just split the sequence of tokens arbitrarily: 1 sentence = 1000 tokens
        sentence, rest = [], b''
        with utils.smart_open(self.fname) as fin:
            while True:
                text = rest + fin.read(8192)  # avoid loading the entire file (=1 line) into RAM
                if text == rest:  # EOF
                    words = utils.to_unicode(text).split()
                    sentence.extend(words)  # return the last chunk of words, too (may be shorter/longer)
                    if sentence:
                        yield sentence
                    break
                last_token = text.rfind(b' ')  # last token may have been split in two... keep for next iteration
                words, rest = (utils.to_unicode(text[:last_token]).split(),
                               text[last_token:].strip()) if last_token >= 0 else ([], text)
                sentence.extend(words)
                while len(sentence) >= self.max_sentence_length:
                    yield sentence[:self.max_sentence_length]
                    sentence = sentence[self.max_sentence_length:]
```
我们在上文中也提及，如果是对于大量的输入语料集或者需要整合磁盘上多个文件夹下的数据，我们可以以迭代器的方式而不是一次性将全部内容读取到内存中来节省 RAM 空间：

```py
class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

sentences = MySentences('/some/directory') # a memory-friendly iterator
model = gensim.models.Word2Vec(sentences)

sentences = MySentences('fzhxx_corpus.txt')
```

# 4. 模型保存与读取

```py
model.save('/tmp/MyModel')
# 前一组方法保存的文件不能利用文本编辑器查看但是保存了训练的全部信息，可以在读取后追加训练
model.save_word2vec_format('/tmp/mymodel.txt',binary = False)
model.save_word2vec_format('/tmp/mymodel.bin.gz',binary = True)
# 后一组方法保存为word2vec文本格式但是保存时丢失了词汇树等部分信息，不能追加训练

model.save('text8.model')
# 2015-02-24 11:19:26,059 : INFO : saving Word2Vec object under text8.model, separately None
# 2015-02-24 11:19:26,060 : INFO : not storing attribute syn0norm
# 2015-02-24 11:19:26,060 : INFO : storing numpy array 'syn0' to text8.model.syn0.npy
# 2015-02-24 11:19:26,742 : INFO : storing numpy array 'syn1' to text8.model.syn1.npy
model1 = Word2Vec.load('text8.model')
model.save_word2vec_format('text.model.bin', binary=True)
# 2015-02-24 11:19:52,341 : INFO : storing 71290x200 projection weights into text.model.bin
model1 = word2vec.Word2Vec.load_word2vec_format('text.model.bin', binary=True)
# 2015-02-24 11:22:08,185 : INFO : loading projection weights from text.model.bin
# 2015-02-24 11:22:10,322 : INFO : loaded (71290, 200) matrix from text.model.bin
# 2015-02-24 11:22:10,322 : INFO : precomputing L2-norms of word weight vectors

# 简要总结：如果训练数据量较大的情况下，采用skip-gram 的效果会更加。

model.save('/tmp/mymodel')  
model.save_word2vec_format('/tmp/mymodel.txt',binary=False)   
# 前一组方法保存可以在读取后追加训练
# 后一组方法保存不能追加训练
```

# 5. 模型预测
Word2Vec 最著名的效果即是以语义化的方式推断出相似词汇：
```py
model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
[('queen', 0.50882536)]
model.doesnt_match("breakfast cereal dinner lunch";.split())
'cereal'
model.similarity('woman', 'man')
0.73723527
model.most_similar(['man'])
[(u'woman', 0.5686948895454407),
 (u'girl', 0.4957364797592163),
 (u'young', 0.4457539916038513),
 (u'luckiest', 0.4420626759529114),
 (u'serpent', 0.42716869711875916),
 (u'girls', 0.42680859565734863),
 (u'smokes', 0.4265017509460449),
 (u'creature', 0.4227582812309265),
 (u'robot', 0.417464017868042),
 (u'mortal', 0.41728296875953674)]
```
如果我们希望直接获取某个单词的向量表示，直接以下标方式访问即可：

model['computer']  # raw NumPy vector of a word
array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)

# 6. 模型评估
Word2Vec 的训练属于无监督模型，并没有太多的类似于监督学习里面的客观评判方式，更多的依赖于端应用。Google 之前公开了20000条左右的语法与语义化训练样本，每一条遵循A is to B as C is to D这个格式，地址在这里:

model.accuracy('/tmp/questions-words.txt')
2014-02-01 22:14:28,387 : INFO : family: 88.9% (304/342)
2014-02-01 22:29:24,006 : INFO : gram1-adjective-to-adverb: 32.4% (263/812)
2014-02-01 22:36:26,528 : INFO : gram2-opposite: 50.3% (191/380)
2014-02-01 23:00:52,406 : INFO : gram3-comparative: 91.7% (1222/1332)
2014-02-01 23:13:48,243 : INFO : gram4-superlative: 87.9% (617/702)
2014-02-01 23:29:52,268 : INFO : gram5-present-participle: 79.4% (691/870)
2014-02-01 23:57:04,965 : INFO : gram7-past-tense: 67.1% (995/1482)
2014-02-02 00:15:18,525 : INFO : gram8-plural: 89.6% (889/992)
2014-02-02 00:28:18,140 : INFO : gram9-plural-verbs: 68.7% (482/702)
2014-02-02 00:28:18,140 : INFO : total: 74.3% (5654/7614)
还是需要强调下，训练集上表现的好也不意味着 Word2Vec 在真实应用中就会表现的很好，还是需要因地制宜。)
