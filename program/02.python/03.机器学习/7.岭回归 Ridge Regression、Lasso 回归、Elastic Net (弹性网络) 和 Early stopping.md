正则化线性模型：岭回归 Ridge Regression、Lasso 回归、Elastic Net (弹性网络) 和 Early stopping - 能找到答案的，只有自己 - CSDN博客 https://blog.csdn.net/tsinghuahui/article/details/80287872

模型正则化(减小自由度)是减少过拟合的方法之一。

对多项式模型来说，正则化可以通过减少阶数来实现。

对线性模型来说，正则化往往通过约束模型的权重来实现。

1. Ridge Regression 岭回归, 又名 Tikhonov regularization
岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项（regularization term）: α∑ni=1θ2iα∑i=1nθi2，以达到在拟合数据的同时，使模型权重尽可能小的目的：

【式-1】岭回归代价函数 
J(θ)=MSE(θ)+α∑i=1nθ2i
J(θ)=MSE(θ)+α∑i=1nθi2

即 
J(θ)=1m∑i=1m(θT⋅x(i)−y(i))2+α∑i=1nθ2i
J(θ)=1m∑i=1m(θT⋅x(i)−y(i))2+α∑i=1nθi2

也即 
J(θ)=(Xθ−y)T(Xθ−y)+α12∥w∥22
J(θ)=(Xθ−y)T(Xθ−y)+α12‖w‖22
α=0α=0：岭回归退化为线性回归
αα 很大：所有的权值都趋于0，最终的优化结果为训练集的均值（a flat line）。
w=[0,θ1,⋯,θn]Tw=[0,θ1,⋯,θn]T
在利用梯度下降法求解时，有：

【式-2】岭回归梯度向量 
∇θMSE(θ)=2mXT(Xθ−y)+αw
∇θMSE(θ)=2mXT(Xθ−y)+αw
令【式-2】取0即可得到闭式解

【式-3】岭回归的闭式解 
θ^=(XTX+αA)−1XTy
θ^=(XTX+αA)−1XTy
A∈Rn×nA∈Rn×n为单位矩阵，左上角的元素为0，与bias term 对应。
【注意】

偏差项θ0θ0并没有包含在正则项中，即sum的下标从1开始，而不是0.
正则项只在模型训练的过程中加在 cost function 中，一旦模型完成训练，在评估模型 performance 的时候 应该使用没有加正则项的形式。=> 训练时的 cost function 和测试时的 performance measure 可以不相同！cost function 需要考虑易于寻优的问题，而 performance measure 应和最终的目标尽可能相近！
由于对输入特征的Scale非常敏感，在进行岭回归分析前对数据进行归一化（例如利用Scikit-Learn的StandardScaler进行预处理）非常重要。（适用于大多数正则化模型）
2. Lasso Regression
Least Absolute shrinkage and Selection Opperation Regression (Lasso Regression)

Lasso 回归是线性回归的另一种正则化版本，正则项为权值向量的ℓ1ℓ1范数：

【式-4】Lasso回归的代价函数 
J(θ)=MSE(θ)+α∑i=1n|θi|
J(θ)=MSE(θ)+α∑i=1n|θi|
【注意 
- Lasso Regression 的代价函数在 θi=0θi=0 处是不可导的. 
- 解决方法：在θi=0θi=0处用一个次梯度向量(subgradient vector)代替梯度，如式-5

【式-5】Lasso Regression 的次梯度向量 


Lasso Regression 有一个很重要的性质是：倾向于完全消除不重要的权重。

例如：当αα 取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0。

也就是说，Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。

3. Elastic Net (弹性网络)
弹性网络在岭回归和Lasso回归中进行了折中，通过 混合比(mix ratio) r 进行控制：

r=0r=0：弹性网络变为岭回归
r=1r=1：弹性网络便诶Lasso回归
【式-6】弹性网络的代价函数 
J(θ)=MSE(θ)+rα∑i=1n|θi|+1−r2α∑i=1nθ2i
J(θ)=MSE(θ)+rα∑i=1n|θi|+1−r2α∑i=1nθi2
一般来说，我们应避免是用朴素线性回归，而应对模型进行一定的正则化处理，那如何选择正则化方法呢？

常用：岭回归
假设只有少部分特征是有用的：弹性网络 或者 Lasso 
一般来说，弹性网络的使用更为广泛。因为在 特征维度高于训练样本数 或者 即为特征是强相关 的情况下，Lasso回归的表现不太稳定。
4. Early Stopping
Early Stopping 也是正则化迭代学习算法（如GD）的方法之一。其做法为：在验证错误率达到最小值的时候停止训练。

beatutiful free lunch 
——Geoffrey Hinton
--------------------- 
作者：元气少女wuqh 
来源：CSDN 
原文：https://blog.csdn.net/tsinghuahui/article/details/80287872 
版权声明：本文为博主原创文章，转载请附上博文链接！