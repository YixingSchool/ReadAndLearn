【机器学习】关于“回归”的完全梳理01 —— 基本回归模型、正则化（脊回归、LASSO、弹性网络） - shengxiaobufu的博客 - CSDN博客 https://blog.csdn.net/shengxiaobufu/article/details/80385264

基本线性回归：
    Duanxx博主的关于线性回归的博客将基本的东西都说明白了，因为博主逻辑有点乱，梳理一下发现，其主要讲的是关于最基本的线性模型的两个方面：

    第一是从概率论的视角给出了线性回归的定义，即“线性回归模型的基本特性就是：模型是参数的线性函数”，这句话给我的震撼非常大，或者说正是因为这句话才让我将Duanxx博主的所有博客看了一遍。因为一直以来我习惯性的理解都是“线性模型的  基本特征是，模型是自变量的线性函数”,而事实上这样的定义才是更为抽象和广义的；此外，将线性模型分为一般线性回归和基于基函数的线性回归（重点提到了多项式曲线拟合（Polynomial Curve Fitting）），给出了介绍。

    第二是线性回归的算法，概括为一次性使用全部数据的批处理技术（Batch techniques）和将数据视为有序序列逐次使用的序列学习算法（Sequential learning，也叫作在线算法（on-line algorithms））；而最常见的批处理算法就是OLS正规方程法，因为它在所有数据的基础上给出了一个简单的数学公式，序列学习算法就是梯度下降和随机梯度下降法。（关于梯度下降法到底是怎么回事，这篇文章讲得比较清楚）

对基本模型的正则化：脊回归、LASSO套索回归、弹性网络
    首先就是关于正则化本身的问题，什么是正则化，为什么要正则化。关于这个问题，这篇文章从过度拟合的角度切入来讲正则化，而在Duanxx博主关于脊回归的文章中，则从多重共线性的角度来讲正则化。事实上我认为这二者虽然并不完全是一回事，但可以肯定是多重共线性的存在，往往会导致过拟合的问题，而同样的，解决这两种问题的思路是类似的，所以一起归并到正则化的问题下。

    总得来讲，根据正则化的思路，衍生出来的基本回归模型的变种一共有岭回归、LASSO套索、弹性网络三种。而浅显地来讲的话，三者的区别就是正则化所用范式不同，从上面两篇文章中也可以看出，正则化有L1和L2两种范式，LASSO用的是L1绝对值  形式，脊回归则是L2，而弹性网络则在二者之间做了一个平衡。但其实我们应该做的是更深入地去理解，这样三种不同的数学处理，在面对实际问题时导致的结果有什么差异。

    在Duanxx博主关于LASSO回归的文章以及Bin的文章中，都讲到了稀疏性的问题。总的来讲就是LASSO，或者说L1正则化可以产生更稀疏的解，帮我们自动地做了一个自变量的选择，这就是LASSO的改进之处。

    而另一个问题就是，在使用了L1正则化之后，由于绝对值无法求导，LASSO是没有直接的解析解的（脊回归有），因此，不得不去思考该用什么样的算法。而Duanxx博主关于LASSO回归的文章就给出了两种常用算法，即坐标轴下降法（coordinate descent）和最小角回归法（ Least Angle Regression），这也是python的sklearn包为LASSO模型提供的两种算法。此外，这篇文章也谈到了LASSO模型可以直接用AIC和BIC作为模型自变量的选取标准的问题，不过本文主要是介绍回归模型本身，关于自变量的选取还有很多其他涉及到的东西，以后应该会专门写一个专题出来。

    此外，对于弹性网络，这篇文章给出了一些介绍，但显然只是翻译了sklearn的官方文档，没有任何自己的解读和理解。实际上不妨将其当作脊回归和LASSO的结合，包括了L1和L2两个正则范式。而对于两个范式对应参数大小的选择，可以更为灵活地适应实际问题。

    最后，正则化应用于回归中，其实是一个收缩参数的过程。而在引入了正则化的思路之后，惩罚项的系数lamda就成为了一个很重要的参数，所以必然会遇到的难题就是怎么在碰到一个实际问题的时候，校准这个参数。在sklearn的官方文档中给的是cross-validation的方式（插一句，这篇文章写了CV是什么，不过我也不好意思说它其实是纯纯地把An Introduction to Statistical Learning这本书翻译了一遍而已），sklearn包给了现成的带cv校准过程的回归模型，分别是：

    CV-岭回归：linear_model.RidgeCV

    CV-LASSO：LassoCV and LassoLarsCV

    CV-弹性网络：ElasticNetCV
--------------------- 
作者：笙箫sx 
来源：CSDN 
原文：https://blog.csdn.net/shengxiaobufu/article/details/80385264 
版权声明：本文为博主原创文章，转载请附上博文链接！