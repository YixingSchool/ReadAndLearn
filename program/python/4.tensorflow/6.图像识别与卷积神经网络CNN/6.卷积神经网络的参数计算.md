卷积神经网络的参数计算 - 绝望的乐园 - CSDN博客 https://blog.csdn.net/qian99/article/details/79008053

前言
这篇文章会简单写一下卷积神经网络上参数的计算方法，然后计算各个常见神经网络的参数。一个是加强对网络结构的了解，另一方面对网络参数的量级有一个大概的认识，也可以当作备忘录，免得想知道的时候还要再算。

## 参数计算方法
全连接的参数计算就不说了，比较简单。
首先，简单说一下卷积网络的参数计算。下图中是一个32x32x3的输入，然后用一个5x5x3的卷积对其中某个位置的计算，这里算的是一个点积，所以输出是一个单独的标量的值。

因为卷积的操作是通过一个滑动窗口实现的，那么通过卷积操作，我们就得到了一个28x28x1的输出。

如果我有6个上面说的filter，那么，我就会得到一个28x28x6的输出。

这就是一个最基础的卷积操作，那么这里用到的参数是多少呢？我们只需要把每个filter的参数累加起来，当然，不要忘了加上bias：5x5x3x6 + 6 = 456

另外一个需要计算的就是进行卷积以后的输出的大小，从下面的图上看就很好理解了，用公式直接算就好了。其中N是输入图像的size，F是filter的size，stride是滑动的步长。

然后从上图中最后一个例子可以看到，stride大于1的时候不一定能整除，这个时候，就需要在原图像上加上一层padding层，这样图像的大小就变化了，然后再用前面的公式算就行了。

然后还有一个maxpooling操作，这个会改变输入输出，但是不会有参数。所以使用和计算卷积一样的公式算就行了。


## LeNet
首先计算一下最简单的LeNet。网络结构如下：

网络层（操作）	输入	filter	stride	padding	输出	计算公式	参数量
Input	32x32x1				32x32x1		0
Conv1	32x32x1	5x5x6	1	0	28x28x6	5x5x1x6+6	156
MaxPool1	28x28x6	2x2	2	0	14x14x6		0
Conv2	14x14x6	5x5x16	1	0	10x10x16	5x5x6x16+16	2416
MaxPool2	10x10x16	2x2	2	0	5x5x16		0
FC1	5x5x16				120	5x5x16x120+120	48120
FC2	120				84	120x84+84	10164
FC3	84				10	84x10+10	850
参数总量： 61706
参数内存消耗： 241.039KB

AlexNet
Alexnet的结构图有些奇怪。但其实是因为要把网络拆分到两个GPU上，才画成了两层，两层的结构是一样的，下面计算的时候的结构相当于合并以后的网络。


网络层（操作）	输入	filter	stride	padding	输出	计算公式	参数量
Input	227x227x3				227x227x3		0
Conv1	227x227x3	11x11x96	4	0	55x55x96	11x11x3x96+96	34944
MaxPool1	55x55x96	3x3	2	0	27x27x96		0
Norm1	27x27x96				27x27x96		0
Conv2	27x27x96	5x5x256	1	2	27x27x256	5x5x96x256+256	614656
MaxPool2	27x27x256	3x3	2	0	13x13x256		0
Norml2	13x13x256				13x13x256		0
Conv3	13x13x256	3x3x384	1	1	13x13x384	3x3x256x384+384	885120
Conv4	13x13x384	3x3x384	1	1	13x13x384	3x3x384x384+384	1327488
Conv5	13x13x384	3x3x256	1	1	13x13x256	3x3x384x256+256	884992
MaxPool3	13x13x256	3x3	2	0	6x6x256		0
FC6	6x6x256				4096	6x6x256x4096+4096	37752832
FC7	4096				4096	4096x4096+4096	16781312
FC8	4096				1000	4096x1000+1000	4097000
参数总量： 62378344
参数内存消耗： 237.9545MB

VGG
VGG常见有16层和19层的，这里以16层为例，下面是模型结构图。


网络层（操作）	输入	filter	stride	padding	输出	计算公式	参数量
Input	224x224x3				224x224x3		0
Conv3-64	224x224x3	3x3x64	1	1	224x224x64	3x3x3x64 + 64	1792
Conv3-64	224x224x64	3x3x64	1	1	224x224x64	3x3x64x64 + 64	36928
MaxPool2	224x224x64	2x2	2	0	112x112x64		0
Conv3-128	112x112x64	3x3x128	1	1	112x112x128	3x3x64x128 + 128	73856
Conv3-128	112x112x128	3x3x128	1	1	112x112x128	3x3x128x128 + 128	147584
MaxPool2	112x112x128	2x2	2	0	56x56x128		0
Conv3-256	56x56x128	3x3x256	1	1	56x56x256	3x3x128x256 + 256	295168
Conv3-256	56x56x256	3x3x256	1	1	56x56x256	3x3x256x256 + 256	590080
Conv3-256	56x56x256	3x3x256	1	1	56x56x256	3x3x256x256 + 256	590080
MaxPool2	56x56x256	2x2	2	0	28x28x256		0
Conv3-512	28x28x256	3x3x512	1	1	28x28x512	3x3x256x512 + 512	1180160
Conv3-512	28x28x512	3x3x512	1	1	28x28x512	3x3x512x512 + 512	2359808
Conv3-512	28x28x512	3x3x512	1	1	28x28x512	3x3x512x512 + 512	2359808
MaxPool2	28x28x512	2x2	2	0	14x14x512		0
Conv3-512	14x14x512	3x3x512	1	1	14x14x512	3x3x512x512 + 512	2359808
Conv3-512	14x14x512	3x3x512	1	1	14x14x512	3x3x512x512 + 512	2359808
Conv3-512	14x14x512	3x3x512	1	1	14x14x512	3x3x512x512 + 512	2359808
MaxPool2	14x14x512	2x2	2	0	7x7x512		0
FC1	7x7x512				4096	7x7x512x4096 + 4096	102764544
FC2	4096				4096	4096*4096 + 4096	16781312
FC3	4096				1000	4096*1000 + 1000	4097000
参数总量： 138357544
参数内存消耗： 527.7921MB

GoogleNet
googlenet 提出了inception的概念，用于增加网络深度和宽度，提高深度神经网络性能。下面是googlenet的网络结构：

inception的结构如下：

可以看出，inception的结构是多个卷积堆叠，组合而成的。

还有，从上面的网络结构中，可以看到一共有三个输出的分类层：

这个是为了解决深层网络训练的时候梯度消失的问题，所以在中间加入了几个全连接层辅助训练。
最后，贴一个论文上给出的模型的结构图：

在这个图上，已经给出了参数的数量和使用的内存，不过我还是说一下inception模块的计算方法和一些注意事项。

首先是输入，输入的size应该为224x224x3
注意第一层的卷积，没有注明padding，直接算的话，结果是不对的，这里的padding计算方法和tensorflow中卷积方法padding参数设置为’SAME’是一样的。简单来说，就是ceil(size/kernel_size)，这个对于下面的计算也是一样的，总之，就是要填适当的0，使得输出结果和上图相对应就是了。
3.在上图中5~10列对应inception module中的各个卷积操作，对应的值是输出的feature的数量，对于maxpool操作，他的padding为2，stride为1。
4.当一个inception模块计算完后，它的输出为各个卷积操作输出的结果连接起来，也就是如果输出分别为28x28x64、28x28x128、28x28x32、28x28x32，那么最终输出就是28x28x(63+128+32+32)。
下面的图给出了inception module内部计算的输出结果。


可以看出googlenet的参数量要比vgg少很多，但是效果确更优秀。

Resnet
关于resnet，我就不打算计算参数了，因为实在量很大，而且实际上，resnet的基本结构也比较简单，计算方法和前面的没什么差别。这里就简单贴一下结构图好了。

可以看出来，如果没有中间一条条连线，其实就是一个很深的普通的卷积网络，中间的连线可以保证梯度可以传递到低层，防止梯度消失的问题。
--------------------- 
作者：qian99 
来源：CSDN 
原文：https://blog.csdn.net/qian99/article/details/79008053 
版权声明：本文为博主原创文章，转载请附上博文链接！