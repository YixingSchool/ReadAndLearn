
## 1. 分类问题和回归问题是监督学习的两大种类

* 分类问题
    1. 分类问题希望解决的是将不同的样本分到事先定义好的类别中
    2. 第3张中判断一个零件是否合格的问题是一个二分类问题
    3. 4.3节中手写体数字识别问题是一个十分类问题

## 2. 交叉熵

* 交叉熵
    1. 交叉熵是一个信息论中的概念，是用来估算平均编码长度的
    2. 给定两个概率分布p和q，通过q来表示p的交叉熵为：H(p,q) = -∑p(x)log q(x)
    3. 交叉熵刻画的是两个概率分布之间的距离，`交叉熵值越小，两个概率分布越接近`

## 3. Softmax

* softmax
    1. Softmax回归将神经网络前向传播得到的结果变成概率分布
    2. 原始神经网络的输出被用作置信度来生成新的输出
    3. 交叉熵函数不是对称的：H(p,q)≠H(q,p)，刻画的是通过概率分布q来表达概率分布p的困难程度

## 4. 三分类问题

* 三分类问题
    1. 正确答案(1,0,0)，预测答案(0.5,0.4,0.1)
        1. 交叉熵：H((1,0,0),(0.5,0.4,0.1))=-(1*log0.5 + 0*log0.4 + 0*log0.1)≈0.3
    2. 正确答案(1,0,0)，预测答案(0.8,0.1,0.1)
        1. 交叉熵：H((1,0,0),(0.8,0.1,0.1))=-(1*log0.8 + 0*log0.1 + 0*log0.1)≈0.1
    3. 第二个预测要优于第一个，第二个交叉熵的值更小

## 5. cross_entropy

* 3.4.5 小节。cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))
    1. y_代表正确结果，y代表预测结果
    2. 第1个操作：tf.clip_by_value函数保证在进行log运算时，不会出现log0错误或者大于1的概率
    3. 第2个操作：tf.log函数对张量中所有元素依次求对数的功能
    4. 第3个操作：*不是矩阵乘法，而是元素之间直接相乘
    5. 通过上面3个运算，完成交叉熵`p(x)logq(x)`的计算
    6. 结果是n*m的二维矩阵，n为一个batch中样例的数量，m为分类的类别数量

## 6. tf.nn.softmax_cross_entropy_with_logits函数

* tf.nn.softmax_cross_entropy_with_logits函数
    1. tensorflow将交叉熵和softmax回归统一封装
    2. cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y, y_)
        1. y_代表标准答案，y代表输出结果

## 7. 回归问题

* 回归问题
    1. 回归问题解决的是对具体数值的预测
    2. 需要预测的不是一个事先定义好的类别，而是一个任意实数
    3. 一般只有一个输出节点
* 损失函数
    1. 均方误差MSE, mean squared error
    2. mse = tf.reduce_mean(tf.square(y_ - y))

