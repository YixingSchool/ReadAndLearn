
## 1. 反向传播算法(backgropagation)和梯度下降算法(gradient decent)

* 梯度下降算法
    * 梯度下降算法主要是用于优化单个参数的取值
* 反向传播算法
    1. 反向传播算法给出一个高效的方式在所有参数上使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小
    2. 根据定义好的损失函数优化神经网络中参数的取值
    3. 梯度下降算法会迭代式更新参数Θ，不断沿着梯度的反方向让参数朝着总损失更小的方向更新

## 2. 参数Θ的取值

* 参数Θ的取值
    1. 参数的梯度可以通过求偏导的方式计算
    2. 学习率 η 定义每次参数更新的幅度，即每次参数移动的幅度 

## 3. 梯度下降算法的例子

![使用梯度下降算法优化函数](使用梯度下降算法优化函数.png)

* 神经网络的优化过程可以分为两个阶段：
    1. 先通过前向传播算法计算得到预测值，并将预测值和真实值做对比得出两者之间的差距
    2. 通过反向传播算法计算损失函数对每一个参数的梯度，再根据梯度和学习率使用梯度下降算法更新每一个参数
* 梯度下降算法并不能保证被优化的函数达到全局最优解
    * 只有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解

![梯度下降算法得不到全局最小值的样例](梯度下降算法得不到全局最小值的样例.png)

## 4. 随机梯度下降

* 随机梯度下降stochastic gradient descent
    1. 这个算法优化的不是在全部训练数据上的损失函数，而是在每一轮迭代中，随机优化某一条训练数据上的损失函数
    2. 无法确保在全部数据上损失函数更小，甚至无法达到局部最优
* batch
    1. 在实际应用中一般综合梯度下降和随机梯度下降的优点
    2. 每次计算一小部分训练数据batch的损失函数

## 5. 损失函数

* 损失函数
    1. 损失函数（Loss function）是用来估量你模型的预测值 f(x)f(x) 与真实值 YY 的不一致程度
    2. 它是一个非负实值函数，通常用 L(Y,f(x))L(Y,f(x)) 来表示。损失函数越小，模型的鲁棒性就越好。
    3. 失函数的定义，衡量模型模型预测的好坏。`损失函数就是用来表现预测与实际数据的差距程度`。

## 参考

1. [机器学习中的损失函数](https://blog.csdn.net/u010976453/article/details/78488279)
2. [关于机器学习中的损失函数。到底什么是损失函数](https://blog.csdn.net/qq_24753293/article/details/78788844)