
## 1. 有监督学习

* 监督学习
    * 需要有一个标注好的训练数据集
    * 在已知答案的标注数据集上，模型给出的预测结果要尽量接近真实的答案

## 2. 反向传播算法backpropagation

* 反向传播算法
    1. 首先需要选取一小部分训练数据batch
    2. batch数据通过前向传播算法得到神经网络模型的预测结果
    3. 计算出当前神经网络模型的预测答案与正确答案之间的差距
    4. 基于预测值和真实值之间的差距，反向传播算法会相应更新神经网络参数的取值

## 3. 通过 placeholder 来指定 batch

```py
import tensorflow as tf

w1= tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))
w2= tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))

x = tf.placeholder(tf.float32, shape=(1, 2), name="input")
a = tf.matmul(x, w1)
y = tf.matmul(a, w2)

with tf.Session().as_default() as sess:
    init_op = tf.global_variables_initializer()
    print(sess.run(init_op))
    # feed_dict指定x的取值。feed_dict是一个字典(map)
    print(sess.run(y, feed_dict={x: [[0.7, 0.9]]}))
    print(y.eval(feed_dict={x: [[.7,.9]]}))
```

* placeholder（占位符）
    * placeholder是TensorFlow的占位符节点，由placeholder方法创建，在sess.run(op,feed_dict={  }  )用feed_dict进行赋值操作，可以将placeholder理解为一种形参。即不像constant那样直接可以使用，需要用户传递常数值。
* 创建方式：
    * X = tf.placeholder(dtype=tf.float32, shape=[144, 10], name='X')
* 参数说明：
    * dtype：数据类型，必填，默认为value的数据类型，传入参数为tensorflow下的枚举值（float32，float64.......）
    * shape：数据形状，选填，不填则随传入数据的形状自行变动，可以在多次调用中传入不同形状的数据
    * name：常量名，选填，默认值不重复，根据创建顺序为（Placeholder，Placeholder_1，Placeholder_2.......）

```py
# -*- coding: utf-8 -*-
import tensorflow as tf
a=tf.placeholder(tf.float32)
b=tf.placeholder(tf.float32)
c=tf.add(a,b)

with tf.Session() as sess:
    print(sess.run(c,feed_dict={a:10,b:30}))  #把10赋给a，30赋给b

# 40.0
```

## 4. 将输入 1*2 矩阵改为 n*2 矩阵

```py
import tensorflow as tf

w1= tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))
w2= tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))

x = tf.placeholder(tf.float32, shape=(3, 2), name="input")
a = tf.matmul(x, w1)
y = tf.matmul(a, w2)

sess = tf.Session()
#使用tf.global_variables_initializer()来初始化所有的变量
init_op = tf.global_variables_initializer()  
sess.run(init_op)

#因为x在定义时指定了n为3，所以在运行前向传播过程时需要提供3个样例数据
print(sess.run(y, feed_dict={x: [[0.7,0.9],[0.1,0.4],[0.5,0.8]]})) 
```

## 5. 损失函数

```py
# 损失函数刻画当前的预测值和真实答案之间的交叉熵(差距)
cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0))) 
train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)
```

* 通过运行sess.run(train_step)就可以对所有在GraphKeys.TRAINABLE_VARIABLES集合中的变量进行优化，使得在当前batch下损失函数更小


## 6. tf.clip_by_value的用法
tf.clip_by_value(A, min, max)：输入一个张量A，把A中的每一个元素的值都压缩在min和max之间。小于min的让它等于min，大于max的元素的值等于max。

```py
import tensorflow as tf;  
import numpy as np;  
  
A = np.array([[1,1,2,4], [3,4,8,5]])  
  
with tf.Session() as sess:  
    print sess.run(tf.clip_by_value(A, 2, 5))

# [[2 2 2 4]
#  [3 4 5 5]]
```

## 7. tf.reduce_mean函数的作用是求平均值

```py
x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)  # 1.5
```

## 8. 优化器

* TensorFlow支持7种不同的优化器，常用有三种：
    * tf.train.GradientDescentOptimizer
    * tf.train.AdamOptimizer
    * tf.train.MomentumOptimizer

## 参考

1. [如何选择优化器 optimizer](https://blog.csdn.net/aliceyangxi1987/article/details/73210204)
2. [tf.clip_by_value的用法](https://blog.csdn.net/lianzhng/article/details/80393471)
3. [Tensorflow一些常用基本概念与函数](https://www.cnblogs.com/wuzhitj/p/6431381.html)
4. [tensorflow的tf.reduce_mean函数](https://blog.csdn.net/liangyihuai/article/details/79050018)
5. [tensorflow入门之placeholder](https://blog.csdn.net/zhangshaoxing1/article/details/68957896/)