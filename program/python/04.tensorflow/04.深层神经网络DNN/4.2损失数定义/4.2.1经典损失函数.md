
## 1. 分类问题和回归问题是监督学习的两大种类

* 分类问题
    1. 分类问题希望解决的是将不同的样本分到事先定义好的类别中
    2. 第3张中判断一个零件是否合格的问题是一个二分类问题
    3. 4.3节中手写体数字识别问题是一个十分类问题

## 2. 交叉熵

* 交叉熵
    1. 交叉熵是一个信息论中的概念，是用来估算平均编码长度的
    2. 给定两个概率分布p和q，通过q来表示p的交叉熵为：H(p,q) = -∑p(x)log q(x)
    3. 交叉熵刻画的是两个概率分布之间的距离，`交叉熵值越小，两个概率分布越接近`

## 3. Softmax

![softmax.png](softmax.png)

1. softmax
    1. Softmax回归将神经网络前向传播得到的结果变成概率分布
    2. 原始神经网络的输出被用作置信度来生成新的输出
    3. 交叉熵函数不是对称的：H(p,q)≠H(q,p)，刻画的是通过概率分布q来表达概率分布p的困难程度
2. tf.nn.softmax()：softmax计算
```py
# -*- coding: utf-8 -*-
import tensorflow as tf
A = [1.0,2.0,3.0,4.0,5.0,6.0]
with tf.Session() as sess:
    print (sess.run(tf.nn.softmax(A)))
# [ 0.00426978  0.01160646  0.03154963  0.08576079  0.23312201  0.63369131]
# 其中所有输出的和为1.
### 等价于以下numpy实现
import numpy as np
aa = np.array([0.1,0.2,0.3])
c = np.exp(aa) / np.sum(np.exp(aa), 0)
print(c)
# [ 0.30060961  0.33222499  0.3671654 ]
```
3. 多类别softmax with交叉熵
    1. tf.nn.softmax_cross_entropy_with_logits && softmax_cross_entropy_with_logits_v2
    2. tensorflow将交叉熵和softmax回归统一封装
    3. cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y, y_)
        1. y_代表标准答案，y代表输出结果
    4. softmax_cross_entropy_with_logits_v2: softmax + 多类别cross-entropy 
```py
a  = tf.constant([[0.1, 0.2, 0.3]],dtype=tf.float32)
label = tf.constant([[0, 0, 1]],dtype=tf.float32)
b = tf.nn.softmax_cross_entropy_with_logits_v2(logits=a, labels=label)

with tf.Session() as sess:
    print(sess.run(b)) # [ 1.00194287]
    print(sess.run(tf.reduce_sum(b))) # 1.00194
### 等价于以下numpy实现: softmax(predictions) + cross_entropy
import numpy as np
def softmax(logits):
    return np.exp(logits)/np.sum(np.exp(logits))

def cross_entropy(label, prediction_softmax):
    result = np.sum(-1*np.log(prediction_softmax)*label)
    return result

prediction = np.array([[0.1, 0.2, 0.3]], dtype=np.float32)
label = np.array([[0, 0, 1]],dtype=np.float32)
c = cross_entropy(label, softmax(prediction))
print(c) # 1.00194
```

## 4. 三分类问题

* 三分类问题
    1. 正确答案(1,0,0)，预测答案(0.5,0.4,0.1)
        1. 交叉熵：H((1,0,0),(0.5,0.4,0.1))=-(1*log0.5 + 0*log0.4 + 0*log0.1)≈0.3
    2. 正确答案(1,0,0)，预测答案(0.8,0.1,0.1)
        1. 交叉熵：H((1,0,0),(0.8,0.1,0.1))=-(1*log0.8 + 0*log0.1 + 0*log0.1)≈0.1
    3. 第二个预测要优于第一个，第二个交叉熵的值更小

## 5. cross_entropy

* 3.4.5 小节。cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))
    1. y_代表正确结果，y代表预测结果
    2. 第1个操作：tf.clip_by_value函数保证在进行log运算时，不会出现log0错误或者大于1的概率
    3. 第2个操作：tf.log函数对张量中所有元素依次求对数的功能
    4. 第3个操作：*不是矩阵乘法，而是元素之间直接相乘
    5. 通过上面3个运算，完成交叉熵`p(x)logq(x)`的计算
    6. 结果是n*m的二维矩阵，n为一个batch中样例的数量，m为分类的类别数量

## 7. 回归问题

* 回归问题
    1. 回归问题解决的是对具体数值的预测
    2. 需要预测的不是一个事先定义好的类别，而是一个任意实数
    3. 一般只有一个输出节点
* 损失函数
    1. 均方误差MSE, mean squared error
    2. mse = tf.reduce_mean(tf.square(y_ - y))

## 参考

1. [softmax](https://blog.csdn.net/caomin1hao/article/details/80587327)