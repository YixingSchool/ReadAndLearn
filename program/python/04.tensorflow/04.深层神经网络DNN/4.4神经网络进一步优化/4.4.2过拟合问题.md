

## 1. 过拟合

* 过拟合
    1. 当一个模型过为复杂之后，它可以很好的“记忆”每一个训练数据中随机噪音的部分而忘记要去“学习”训练数据中通用的趋势
    2. 过度拟合了训练数据中的噪音而忽视了问题的整体规律

## 2. 正则化

1. 思想
    1. 正则化的思想就是在损失函数中`加入刻画模型复杂程度的指标`
    2. 通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪音
2. 种类
    1. L1正则化
        1. 让参数变得更稀疏。会有更多的参数变成0
        2. L1正则化的计算公式不可导
    2. L2正则化
        1. 不会让参数变得稀疏
        2. 可导

## 3. L2正则化的损失函数

```py
w = tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))
y = tf.matmul(x, w)

# loss由两部分组成，第一部分为4.2.1小结总的均方误差损失函数
# 第二部分是正则化，防止模型过度模拟训练数据中的随机噪音
# lambda参数表示了正则化项的权重
loss = tf.reduce_mean(tf.square(y_ - y)) + tf.contrib.layers.l2_regularizer(lambda)(w)
```

![L1&L2正则化](L1&L2正则化.png)

## 4. add_to_collection

1. tf.add_to_collection函数可以将资源加入一个或多个集合中
2. 然后通过tf.get_collection获取一个集合里面的所有资源

```py
sess=tf.InteractiveSession()
#初始化2个Variable
v1=tf.Variable(tf.constant(1))
v2=tf.Variable(tf.constant(1))
#设置保存到collection的name为collection
name='collection'
#把v1和v2添加到默认graph的collection中
tf.add_to_collection(name,v1)
tf.add_to_collection(name,v2)
#获得名为name的集合
c1 = tf.get_collection(name)
tf.global_variables_initializer().run()
print("the first collection: %s"%sess.run(c1))
#修改v1和v2的值,必须使用eval()或run()进行执行
tf.assign(v1,tf.constant(3)).eval()
tf.assign(v2,tf.constant(4)).eval()
#再次查看collection中的值
c2 = tf.get_collection(name)
print("the second collection: %s"%sess.run(c2))
print("the sum of collection: %s"%sess.run(tf.add_n(c2))
#resut:
#the first collection: [1, 1]
#the second collection: [3, 4]
#the sum of collection: 7
```


## 5. 集合计算5层神经网络带L2正则化的损失函数

```py
import tensorflow as tf

# 获取一层神经网络边上的权重，并将整个权重的L2这个正则化损失加入名称为'losses'的集合中
def get_weight(shape, var_lambda):
    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)
    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(var_lambda)(w))
    return w

x = tf.placeholder(tf.float32, shape=(None, 2))
y_ = tf.placeholder(tf.float32, shape=(None, 1))

# 每层节点的个数
layer_dimension = [2,10,5,3,1]
# 神经网络的层数
n_layers = len(layer_dimension)

cur_layer = x
in_dimension = layer_dimension[0]

# 循环生成网络结构，输入层X[None, 2]，隐藏层W1[2, 10]、W2[10, 5]、W3[5, 3]，输出层Y[None, 1]
for i in range(1, n_layers):
    out_dimension = layer_dimension[i]
    weight = get_weight([in_dimension, out_dimension], 0.003)
    bias = tf.Variable(tf.constant(0.1, shape=[out_dimension]))
    cur_layer = tf.nn.elu(tf.matmul(cur_layer, weight) + bias)
    in_dimension = layer_dimension[i]

y= cur_layer

# 损失函数的定义。
mse_loss = tf.reduce_sum(tf.pow(y_ - y, 2)) / dataset_size
tf.add_to_collection('losses', mse_loss)
loss = tf.add_n(tf.get_collection('losses'))
```

## 参考

1. [tensorflow：3.1)add_to_collection和L2正则化](https://blog.csdn.net/jiangpeng59/article/details/72852184)