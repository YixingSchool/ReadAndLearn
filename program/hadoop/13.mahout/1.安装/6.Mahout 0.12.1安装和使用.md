https://blog.csdn.net/dst1213/article/details/51584105

Ubuntu下基于Hadoop 2.6.2的Mahout 0.12.1安装和使用
环境：
Ubuntu Server 14.04.04 amd64 Hadoop 2.6.2 
参考：
http://itindex.net/detail/49323-mahout-%E5%AD%A6%E4%B9%A0-mahout
新版的配置有略微改动


1、下载
http://mirror.bit.edu.cn/apache/mahout/0.12.1/
apache-mahout-distribution-0.12.1.tar.gz


2、解压
tar -zxvf mahout-distribution-0.12.1.tar.gz


3、环境变量
3.1、配置Mahout环境变量
# set mahout environment
export MAHOUT_HOME=/usr/local/mahout/mahout-distribution-0.12.1
export MAHOUT_CONF_DIR=$MAHOUT_HOME/conf
export PATH=$MAHOUT_HOME/conf:$MAHOUT_HOME/bin:$PATH
3.2、配置Mahout所需的Hadoop环境变量
 # set hadoop environment
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.6.2 
export MAHOUT_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export HADOOP_HOME_WARN_SUPPRESS=not_null


## 四、验证Mahout是否安装成功
$mahout，执行
若列出一些算法，则成功
root@spark:/usr/local/mahout/apache-mahout-distribution-0.12.1# mahout
MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Running on hadoop, using /usr/local/hadoop/hadoop-2.6.2/bin/hadoop and HADOOP_CONF_DIR=/usr/local/hadoop/hadoop-2.6.2/conf
MAHOUT-JOB: /usr/local/mahout/apache-mahout-distribution-0.12.1/mahout-examples-0.12.1-job.jar
An example program must be given as the first argument.
Valid program names are:
  arff.vector: : Generate Vectors from an ARFF file or directory
  baumwelch: : Baum-Welch algorithm for unsupervised HMM training
  canopy: : Canopy clustering
  cat: : Print a file or resource as the logistic regression models would see it
  cleansvd: : Cleanup and verification of SVD output
  clusterdump: : Dump cluster output to text
  clusterpp: : Groups Clustering Output In Clusters
  cmdump: : Dump confusion matrix in HTML or text formats
。。。


## 五、Mahout使用（一）
5.1、启动Hadoop
（Hadoop安装可参考本人其他博文的链接）
/usr/local/hadoop/hadoop-2.6.2/sbin/start-dfs.sh
/usr/local/hadoop/hadoop-2.6.2/sbin/start-yarn.sh
./mr-jobhistory-daemon.sh start historyserver
5.2、下载测试数据
http://archive.ics.uci.edu/ml/databases/synthetic_control/，链接中的synthetic_control.data
5.3、上传测试数据
root@spark:~# hadoop fs -put /home/alex/pcshare/XData/synthetic_control.data /user/root/testdata
5.4  使用Mahout中的kmeans聚类算法

执行命令：

mahout -core  org.apache.mahout.clustering.syntheticcontrol.kmeans.Job
等待几分钟。。。 
出现类似下边的说明成功了。。。结果自动打印出来了
1.0 : [distance=65.25867095278808]: [30.573,41.074,44.979,44.922,43.272,39.713,33.097,31.012,26.03,22.191,16.82,21.089,29.101,27.763,41.43,42.245,35.947,44.868,35.821,24.775,29.244,20.686,12.381,25.885,18.873,26.381,39.696,40.666,40.693,35.249,31.458,28.821,24.015,19.608,13.017,13.592,26.841,26.265,26.931,41.894,38.723,42.73,39.036,30.817,24.18,25.618,14.164,16.674,14.107,24.789,29.088,31.482,36.327,46.833,46.544,38.817,32.532,24.353,19.72,12.751]
16/06/04 13:58:22 INFO ClusterDumper: Wrote 6 clusters
16/06/04 13:58:22 INFO MahoutDriver: Program took 770182 ms (Minutes: 12.836483333333334)
root@spark:~# 
5.5 查看聚类结果
$hadoop fs -ls /user/root/output，查看聚类结果。 
root@spark:~# hadoop fs -ls /user/root/output
Found 15 items
-rw-r--r--   1 root supergroup        194 2016-06-04 13:57 /user/root/output/_policy
drwxr-xr-x   - root supergroup          0 2016-06-04 13:58 /user/root/output/clusteredPoints
drwxr-xr-x   - root supergroup          0 2016-06-04 13:47 /user/root/output/clusters-0
drwxr-xr-x   - root supergroup          0 2016-06-04 13:49 /user/root/output/clusters-1
drwxr-xr-x   - root supergroup          0 2016-06-04 13:57 /user/root/output/clusters-10-final
drwxr-xr-x   - root supergroup          0 2016-06-04 13:50 /user/root/output/clusters-2
drwxr-xr-x   - root supergroup          0 2016-06-04 13:51 /user/root/output/clusters-3
drwxr-xr-x   - root supergroup          0 2016-06-04 13:52 /user/root/output/clusters-4
drwxr-xr-x   - root supergroup          0 2016-06-04 13:53 /user/root/output/clusters-5
drwxr-xr-x   - root supergroup          0 2016-06-04 13:54 /user/root/output/clusters-6
drwxr-xr-x   - root supergroup          0 2016-06-04 13:55 /user/root/output/clusters-7
drwxr-xr-x   - root supergroup          0 2016-06-04 13:56 /user/root/output/clusters-8
drwxr-xr-x   - root supergroup          0 2016-06-04 13:57 /user/root/output/clusters-9
drwxr-xr-x   - root supergroup          0 2016-06-04 13:47 /user/root/output/data
drwxr-xr-x   - root supergroup          0 2016-06-04 13:47 /user/root/output/random-seeds
root@spark:~# 


六、Mahout使用（二）
KMeans聚类
主要参考，http://www.coder4.com/archives/4181，不过更新了一点新版的命令
http://my.oschina.net/endeavour/blog/491518?p={{totalPage}}
数据下载：
http://www.daviddlewis.com/resources/testcollections/reuters21578/
http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz
6.1解压
先用tar =xvzf reuters21578.tar.gz，（在windows下也可以用winrar解压）
root@spark:/home/alex# ls
pcshare  reuters21578  seed.txt  t1.log  test.log  xdata  xsetups
6.2用mahout ExtractReuters抽取数据
root@spark:/home/alex# mahout org.apache.lucene.benchmark.utils.ExtractReuters ./reuters21578/ ./reuters-out
MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Running on hadoop, using /usr/local/hadoop/hadoop-2.6.2/bin/hadoop and HADOOP_CONF_DIR=/usr/local/hadoop/hadoop-2.6.2/etc/hadoop
MAHOUT-JOB: /usr/local/mahout/apache-mahout-distribution-0.12.1/mahout-examples-0.12.1-job.jar
16/06/05 22:23:50 WARN MahoutDriver: No org.apache.lucene.benchmark.utils.ExtractReuters.props found on classpath, will use command-line arguments only
Deleting all files in /home/alex/./reuters-out-tmp
16/06/05 22:24:06 INFO MahoutDriver: Program took 15945 ms (Minutes: 0.26575)
root@spark:/home/alex# ls
pcshare  reuters21578  reuters-out  seed.txt  t1.log  test.log  xdata  xsetups
6.3转为序列化文件
6.3.1 local mode
mahout seqdirectory -i file://$(pwd)/reuters-out/ -o file://$(pwd)/reuters-seq/ -c UTF-8 -chunk 64 -xm sequential
上述命令蕴含了2个大坑，在其他文档中均没有仔细说明：
(1) -xm sequential，表示在本地执行，而不是用MapReduce执行。如果是后者，我们势必要将这些小文件上传到HDFS上，那样的话，还要SequenceFile做甚……
(2) 然而seqdirectory在执行的时候，并不因为十本地模式，就在本地文件系统上寻找。而是根据-i -o的文件系统前缀来判断文件位置。也就是说，默认情况，依然十在HDFS上查找的……所以，这个file://的前缀是非常有必要的。
结果：
root@spark:/home/alex# mahout seqdirectory -i file://$(pwd)/reuters-out/ -o file://$(pwd)/reuters-seq/ -c UTF-8 -chunk 64 -xm sequential
MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Running on hadoop, using /usr/local/hadoop/hadoop-2.6.2/bin/hadoop and HADOOP_CONF_DIR=/usr/local/hadoop/hadoop-2.6.2/etc/hadoop
MAHOUT-JOB: /usr/local/mahout/apache-mahout-distribution-0.12.1/mahout-examples-0.12.1-job.jar
16/06/05 22:30:51 INFO AbstractJob: Command line arguments: {--charset=[UTF-8], --chunkSize=[64], --endPhase=[2147483647], --fileFilterClass=[org.apache.mahout.text.PrefixAdditionFilter], --input=[file:///home/alex/reuters-out/], --keyPrefix=[], --method=[sequential], --output=[file:///home/alex/reuters-seq/], --startPhase=[0], --tempDir=[temp]}
16/06/05 22:37:32 INFO MahoutDriver: Program took 401032 ms (Minutes: 6.683883333333333
root@spark:/home/alex# ls
pcshare       reuters-out  seed.txt  test.log  xsetups
reuters21578  reuters-seq  t1.log    xdata
6.3.2 hdfs mode
先把6.2的文件用hadoop fs -put ./reuters-out /reuters-out传到hdfs上，然后
mahout seqdirectory -c UTF-8
-i /reuters-out
-o reuters-seq
6.4向量化
先上传到hdfs
root@spark:/home/alex# hadoop fs -put reuters-seq /reuters-seq
root@spark:/home/alex# hadoop fs -ls /
Found 17 items
drwxr-xr-x   - root supergroup          0 2016-06-05 22:41 /reuters-seq
向量化
mahout seq2sparse -i /reuters-seq -o /reuters-sparse -ow --weight tfidf --maxDFPercent 85 --namedVector
输入和输出不解释了。在Mahout中的向量类型可以称为sparse。
参数说明如下：
-ow( 或 –overwrite)：即使输出目录存在，依然覆盖。
–weight(或 -wt) tfidf：权重公式，大家都懂的。其他可选的有tf (当LDA时建议使用)。
–maxDFPercent(或 -x) 85：过滤高频词，当DF大于85%时，将不在作为词特征输出到向量中。
–namedVector (或-nv)：向量会输出附加信息。
其他可能有用的选项：
–analyzerName(或-a)：指定其他分词器。
–minDF：最小DF阈值。
–minSupport：最小的支持度阈值，默认为2。
–maxNGramSize(或-ng)：是否创建ngram，默认为1。建议一般设定到2就够了。
–minLLR(或 -ml)：The minimum Log Likelihood Ratio。默认为1.0。当设定了-ng > 1后，建议设置为较大的值，只过滤有意义的N-Gram。
–logNormalize(或 -lnorm)：是否对输出向量做Log变换。
–norm(或 -n)：是否对输出向量做p-norm变换，默认不变换。
结果：
16/06/05 22:56:43 INFO HadoopUtil: Deleting /reuters-sparse/partial-vectors-0
16/06/05 22:56:43 INFO MahoutDriver: Program took 774045 ms (Minutes: 12.90075)
root@spark:/home/alex# hadoop fs -ls /reuters-sparse
Found 7 items
drwxr-xr-x   - root supergroup          0 2016-06-05 22:51 /reuters-sparse/df-count
-rw-r--r--   1 root supergroup     824086 2016-06-05 22:48 /reuters-sparse/dictionary.file-0
-rw-r--r--   1 root supergroup     844593 2016-06-05 22:52 /reuters-sparse/frequency.file-0
drwxr-xr-x   - root supergroup          0 2016-06-05 22:54 /reuters-sparse/tf-vectors
drwxr-xr-x   - root supergroup          0 2016-06-05 22:56 /reuters-sparse/tfidf-vectors
drwxr-xr-x   - root supergroup          0 2016-06-05 22:45 /reuters-sparse/tokenized-documents
drwxr-xr-x   - root supergroup          0 2016-06-05 22:47 /reuters-sparse/wordcount
6.5 KMeans聚类
mahout kmeans -i /reuters-sparse/tfidf-vectors -c /reuters-kmeans-clusters -o /reuters-kmeans -k 20 -dm org.apache.mahout.common.distance.CosineDistanceMeasure -x 200 -ow --clustering
参数说明如下：
-i：输入为上面产出的tfidf向量。
-o：每一轮迭代的结果将输出在这里。
-k：几个簇。
-c：这是一个神奇的变量。若不设定k，则用这个目录里面的点，作为聚类中心点。否则，随机选择k个点，作为中心点。
-dm：距离公式，文本类型推荐用cosine距离。
-x ：最大迭代次数。
–clustering：在mapreduce模式运行。
–convergenceDelta：迭代收敛阈值，默认0.5，对于Cosine来说略大。
结果：
16/06/05 23:06:06 INFO MahoutDriver: Program took 282166 ms (Minutes: 4.70285)
输出1，初始随机选择的中心点：
root@spark:/home/alex# hadoop fs -ls /reuters-kmeans-clusters
Found 1 items
-rw-r--r--   1 root supergroup      20692 2016-06-05 23:01 /reuters-kmeans-clusters/part-randomSeed
输出2，聚类过程、结果：
root@spark:/home/alex# hadoop fs -ls /reuters-kmeans
Found 5 items
-rw-r--r--   1 root supergroup        194 2016-06-05 23:05 /reuters-kmeans/_policy
drwxr-xr-x   - root supergroup          0 2016-06-05 23:06 /reuters-kmeans/clusteredPoints
drwxrwxrwx   - root supergroup          0 2016-06-05 23:01 /reuters-kmeans/clusters-0
drwxr-xr-x   - root supergroup          0 2016-06-05 23:03 /reuters-kmeans/clusters-1
drwxr-xr-x   - root supergroup          0 2016-06-05 23:05 /reuters-kmeans/clusters-2-final
查看簇结果
首先，用clusterdump，来查看k(20)个簇的信息。
root@spark:/home/alex# hadoop fs -get /reuters-kmeans/ ./
查看簇信息
root@spark:/home/alex# mahout clusterdump -i /reuters-kmeans/clusters-2-final -d /reuters-sparse/dictionary.file-0 -dt sequencefile -o ./reuters-kmeans-cluster-dump/ -n 20
要说明的是，clusterdump似乎只能在本地执行……所以先把数据下载到本地吧。上边那个-o是本地路径，前边两个是hdfs路径。
参数说明：
-i ：我们只看最终迭代生成的簇结果。
-d ：使用 词 -> 词id 映射，使得我们输出结果中，可以直接显示每个簇，权重最高的词文本，而不是词id。
-dt：上面映射类型，由于我们是seqdictionary生成的，so。。
-o：最终产出目录
-n：每个簇，只输出20个权重最高的词。
看看dump结果吧：
一共有20行，表示20个簇。每行形如：
结果：
root@spark:/home/alex# ls
pcshare       reuters-kmeans               reuters-out  seed.txt  test.log  xsetups
reuters21578  reuters-kmeans-cluster-dump  reuters-seq  t1.log    xdata
root@spark:/home/alex# vi reuters-kmeans-cluster-dump 


{"identifier":"VL-3240","r":[{"0":1.072},{"0.01":0.409},{"0.02":0.313},{"0.07":0.491},{"0.1":0.699},{"0.10":0.923},{"0.11":0.796},{"0.12":0.715},{"0.125":0.313},{"0.13":0.738},{"0.15":0.652},{"0.16":0.433},{"0.17":0.433},{"0.18":0.301},{"0.19":0.452},{"0.2":0.613},{"0.20":0.405},{"0.21":0.466},{"0.22":0.796},{"0.23":0.685},{"0.24":0.452},{"0.25":0.662},{"0.28":0.571},{"0.29":0.466},{"0.3":0.373},{"0.30":0.675},{"0.31":0.485},{"0.32":0.485},{"0.33":0.602},{"0.34":0.32},{"0.35":0.886},{"0.375":0.33},{"0.38":0.433},{"0.39":0.81},{"0.4":0.309},{"0.40":0.633},{"0.41":0.33},{"0.43":0.466},{"0.44":0.659},{"0.45":0.409},{"0.46":0.32},{"0.49":0.659},{"0.5":0.353},{"0.50":0.755},{"0.51":0.433},{"0.52":1.042},{"0.53":0.639},{"0.55":0.613},{"0.59":0.32},{"0.6":0.32},{"0.60":0.501},{"0.61":0.715},{"0.63":0.466},{"0.65":0.613},{"0.66":0.659},{"0.7":0.492},{"0.72":0.466},{"0.73":0.466},{"0.75":0.384},{"0.76":0.715},{"0.77":0.53},{"0.78":0.442},{"0.79":0.466},{"0.8":0.4
。。。
root@spark:/home/alex# vi reuters-kmeans-cluster-dump 


        Top Terms:
                tonnes                                  =>   4.616685099484926
                wheat                                   =>  2.4975221396023555
                said                                    =>   1.823860074602947
                u.s                                     =>  1.7899877993194988
                corn                                    =>  1.7751955093944525
                agriculture                             =>  1.7748332692679425
                crop                                    =>  1.7612196082261729
                usda                                    =>  1.7265009986265731
                87                                      =>  1.7197907841816247
                grain                                   =>  1.6382993807505923
                1986                                    =>  1.6183747005356446
                department                              =>  1.4992590311640885


其中前面的3240是簇的ID，n即簇中有这么多个文档。c向量是簇中心点向量，格式为 词文本:权重(点坐标)，r是簇的半径向量，格式为 词文本:半径。


下面的Top Terms是簇中选取出来的特征词。


查看聚类结果


其实，聚类结果中，更重要的是，文档被聚到了哪个类。


遗憾的是，在很多资料中，都没有说明这一点。前文我们已经提到了，簇id -> 文档id的结果，保存在了clusteredPoints下面。这也是mahout内置类型存储的。我们可以用seqdumper命令查看。

mahout seqdumper -i /reuters-kmeans/clusteredPoints/

其中，-d和-dt的原因同clusterdump。


如果不指定-o，默认输出到屏幕，输出结果为形如：
Key: 3533: Value: wt: 1.0 distance: 0.5482928103119538  vec: [{"834":8.677},{"1555":7.453},{"2689":3.135},{"3730":3.051},{"4397":2.9},{"4747":4.021},{"7711":4.54},{"10344":4.808},{"12157":4.993},{"19031":4.827},{"20362":1.988},{"23135":4.284},{"25508":6.227},{"26109":4.315},{"29802":4.378},{"29996":4.846},{"30234":4.959},{"30377":3.82},{"30567":3.545},{"30839":3.567},{"33426":3.249},{"33834":9.624},{"34837":5.249},{"36387":4.446},{"39389":11.363},{"39463":9.881},{"39538":4.789},{"40723":7.031}]
Count: 21578
16/06/05 23:26:36 INFO MahoutDriver: Program took 37830 ms (Minutes: 0.6305)


其实，这个输出是一个SequenceFile，大家自己写程序也可以读出来的。


Key是ClusterID，上面clusterdump的时候，已经说了。


Value是文档的聚类结果：wt是文档属于簇的概率，对于kmeans总是1.0，/reut2-000.sgm-0.txt就是文档标志啦，前面seqdirectionary的-nv起作用了，再后面的就是这个点的各个词id和权重了。




七、Mahout使用（三）：
数据如下，代码待续。。。
http://qwone.com/~jason/20Newsgroups/
http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz