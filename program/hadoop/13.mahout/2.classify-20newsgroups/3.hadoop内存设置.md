hadoop内存大小设置问题【转】 - XGogo - 博客园 https://www.cnblogs.com/seaspring/articles/7090385.html

前面博客里面提到，运行一次Hadoop的时候出现Java heap error。字面意思分配堆的时候出现错误，我们知道应用程序的动态内存的分配均在堆里面。这里提示堆错误，那必然是内存不够用了。那么这个namenode内存的大小该怎么取值呢？

namenode管理着集群里面所有文件的信息。简单根据文件信息给出一个准确计算内存大小的公式是不现实的。

hadoop默认namenode内存的大小为1000M，这个值对于数百万的文件来说是足够的，可以保守地设置每百万数据块需要1000MB内存。

例如，有这样一个场景，一个含有200个节点的集群，每个节点有一个24TB的磁盘，hadoop的block的大小为128MB，有三份拷贝总共需要块的数目大概在200万或者更多，那么内存大致需要多少？

首先计算可以有多少块：

（200*24000000MB)/(128MB*3)=12500,000。

然后保守估计需要多少内存：

12500,000*1000MB/1000,000=12,500MB

从上面的计算结果看出，将namenode内存的大小设置为12,000MB这个数量级别可以满足。

 

计算大致的值之后，怎么设置呢？

hadoop配置文件，hadoop-env.sh中有个选项HADOOP_NAMENODE_OPTS，此JVM选项是用来设置内存大小的。比如：

 

[plain] view plain copy
 
HADOOP_NAMENODE_OPTS=-Xmx2000m  
那么就是给namenode分配了2000MB的空间。

 

如果改变了namenode的内存大小，那么secondarynamenode的内存的大小同样也要改变，其选项是HADOOP_SECONDARYNAMENODE_OPTS。