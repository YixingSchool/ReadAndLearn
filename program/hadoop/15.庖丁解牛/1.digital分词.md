

http://f.dataguru.cn/thread-461215-1-1.html

1 建立一个项目，调用庖丁分词器，对digital下的其中一个文本文件分词，把主要代码和输出结果截图

这一题看似简单,但是在具体操作的时候还是遇到了很多的问题:
(1) 在while中使用increment()这个函数的时候,提示没有这个函数    通过替换老师给的paoding-analysis.jar和lucene-core-3.1.0.jar,解决了问题.
(2)一直提示找不到dic这个目录,可是我明明放在了src的同级目录啊,最后发现,必须要把dic放在src目录里面才行,我觉得很诡异.


上代码:
import Java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;


import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;


import net.paoding.analysis.analyzer.PaodingAnalyzer;




public class TokenizerFile {
        public static void main(String[] args) throws FileNotFoundException{
                File f = new File("/home/david/Documents/Dataguru/HadoopInstance/hadoop_dev_07/homework/digital/computer/","computer3");
                PaodingAnalyzer pa = new PaodingAnalyzer();
                FileReader fr = new FileReader(f);
                TokenStream ts = pa.tokenStream("", fr);
                try {
                        while(ts.incrementToken()){
                                CharTermAttribute ca = ts.getAttribute(CharTermAttribute.class);
                                System.out.print(ca.toString() + "  ");
                        }
                } catch (IOException e) {
                        // TODO Auto-generated catch block
                        e.printStackTrace();
                }
        }
}
截图:这个是将结果复制下来的结果,处理的是digital/computer/computer3  这歌文件

  