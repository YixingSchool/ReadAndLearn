hadoop 添加本机文件夹到HDFS目录以及一些简单命令 - CSDN博客 https://blog.csdn.net/macanv/article/details/52729625

写完mapreduce程序后，将jar包上传到集群中run,会出现如下异常

Exception in thread "main" java.lang.ClassNotFoundException: /input
1
原因是input目录是自己的本地目录，而不是HDFS的目录，会提示你hdfs://node1:9000/input path 是不存在的，解决方法就是将本地的input目录加到HDFS目录中，添加方法我们可以参考官方的文档： 
hadoop shell 官方文档

下面举几个使用shell的例子

1. 添加本地文件到hdfs目录
hadoop fs -put /home/hadoop/input hdfs://node1:9000/tmp
上面命令的hadoop fs -put 后面的第一个参数是本地路径，第二个参数是hadoop HDFS上的路径，意思就是将本地路径加载到HDFS上。

2. 创建文件夹
在hadoop的HDFS上创建文件夹和在she’ll上创建文件夹一样：

hadoop fs -mkdir hdfs://node1:9000/tmp/input
上面命令在HDFS的tmp目录下穿件了input文件夹

删除文件夹
每一次运行mapreduce程序之前，要确保output目录是不存在或者空的，那么，我们就要把文件夹删除掉，删除的方法如下：
hadoop fs -rmr hdfs://node1:9000/tmp/output
上面的命令就把output文件夹删除了，-rmr是一个递归删除操作，会删除该文件夹下面的所有文件以及文件夹。也可以选用-rm ,单个删除。

写在最后
http://blog.csdn.net/fansy1990/article/details/21101667?utm_source=tuicool&utm_medium=referral 
这篇博客在hadoop1.x下对比了put,copyFromLocal这两个命令的效率，喜欢的朋友可以去看看，其得出的结论是后者性能更好，没有做验证，不做评论。 
上面写的东西自己在遇到问题的时候，找了些方法，解决问题的过程是痛苦的，解决成功是快乐的。