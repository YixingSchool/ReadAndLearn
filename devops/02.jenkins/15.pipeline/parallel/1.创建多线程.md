

pipeline能够使用parallel来同时执行多个任务。 parallel的调用需要传入map类型作为参数，map的key为名字，value为要执行的groovy脚本。
为了测试parallel的运行，可以安装parallel test executor插件。此插件可以将运行缓慢的测试分割splitTests。

用下面的脚本新建pipeline job：

```groovy
node('remote') {
  git 'https://github.com/jenkinsci/parallel-test-executor-plugin-sample.git'
  stash name: 'sources', includes: 'pom.xml,src/'
}
def splits = splitTests count(2)
def branches = [:]
for (int i = 0; i < splits.size(); i++) {
  def index = i // fresh variable per iteration; i will be mutated
  branches["split${i}"] = {
    node('remote') {
      deleteDir()
      unstash 'sources'
      def exclusions = splits.get(index);
      writeFile file: 'exclusions.txt', text: exclusions.join("\n")
      sh "${tool 'M3'}/bin/mvn -B -Dmaven.test.failure.ignore test"
      junit 'target/surefire-reports/*.xml'
    }
  }
}
parallel branches
```

 

如果遇到RejectedAccessException错误，需要管理员approve权限staticMethod org.codehaus.groovy.runtime.ScriptBytecodeAdapter compareLessThan java.lang.Object java.lang.Object。

当第一次运行上面的pipeline job的时候，所有的测试顺序执行。当第二次或以后执行的时候，splitTests将会将所有的测试分割为大概等价的两份，然后两个task并行运行。如果两个task运行在不同的slave上，则可以看到job总的时间将会减半。

下面的等价语句用来打包pom.xml和源代码：
archive 'pom.xml, src/'
step([$class: 'ArtifactArchiver', artifacts: 'pom.xml, src/'])

我们可以看到prallel里的语句使用了node，这意味着并行执行的任务将会在新的node/slave上执行，且使用不同的workspace，为了确保所有的node和workspace使用相同的代码，所以才有了前面的打包archive和parallel里的解包unarchive。

上面的例子中我们可以看到同一个pipeline job里可以使用多个node，多个node会有不同的workspace，我们需要确保每个workspace的内容都是我们想要的内容。

另一个问题，如果在pipeline中使用env，环境变量的修改会在整个pipeline起作用，如果只修改parallel并行的线程的变量，可以使用withEnv。

在使用了parallel的console log里，并行的log都混在了一起，需要在job的pipeline steps页面查看按逻辑分割的更情况的log。


## 参考
1. https://www.cnblogs.com/itech/p/5646219.html
2. https://github.com/jenkinsci/pipeline-plugin/blob/master/TUTORIAL.md
3. https://github.com/ciandcd